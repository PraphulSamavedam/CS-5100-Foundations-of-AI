{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Foundations of AI","text":"<p>This repository contains projects developed for CS-5100 Foundations of Artificial Intelligence course. Each project explores fundamental AI concepts through practical implementations in the Pacman game environment.   NOTE: Only the underlying algorithms are coded, the environment setup is forked from the UC Berkley's program</p> <p>Institution: Northeastern University   Focus Areas: Search algorithms, probabilistic reasoning, reinforcement learning, multi-agent systems, game playing    </p>"},{"location":"#technologies","title":"Technologies","text":"<ul> <li>Python 3.6+ - Primary programming language for all projects</li> <li>Pacman Game Engine - UC Berkeley's educational AI framework</li> <li>Standard libraries - util, math, random (no external AI/ML frameworks)</li> <li>Autograder system - Automated testing and validation</li> <li>Classic AI algorithms (search, inference, learning)</li> <li>Game tree search and adversarial reasoning</li> </ul>"},{"location":"#projects-overview","title":"Projects Overview","text":""},{"location":"#search-algorithms","title":"Search Algorithms","text":"<p>Classic uninformed and informed search for pathfinding in maze environments.</p> <p>Key Features: DFS, BFS, UCS, A* search with custom heuristics   Concepts: State space search, frontier management, admissible heuristics, optimality analysis   Performance: A* expands 2-3x fewer nodes than uninformed search    </p>"},{"location":"#probabilistic-tracking","title":"Probabilistic Tracking","text":"<p>Bayesian inference and particle filtering for tracking hidden ghosts with noisy sensors.</p> <p>Key Features: Exact inference, particle filtering, joint distributions, belief propagation   Concepts: Bayesian Networks, Hidden Markov Models, forward algorithm, importance sampling   Performance: Real-time tracking at 30+ FPS with 100-500 particles    </p>"},{"location":"#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Value-based learning methods for discovering optimal policies through experience.</p> <p>Key Features: Value Iteration, Q-Learning, Approximate Q-Learning, epsilon-greedy exploration   Concepts: Markov Decision Processes, Bellman equations, temporal difference learning, function approximation   Performance: Converges to optimal policy with proper hyperparameters    </p>"},{"location":"#multi-agent-systems","title":"Multi-Agent Systems","text":"<p>Adversarial search algorithms for game playing against intelligent opponents.</p> <p>Key Features: Minimax, Alpha-Beta pruning, Expectimax, evaluation functions   Concepts: Game trees, adversarial search, zero-sum games, optimization through pruning   Performance: 2-10x speedup with alpha-beta pruning (typically 3-5x)    </p>"},{"location":"#repository-structure","title":"Repository Structure","text":"<pre><code>CS-5100-Foundations-of-AI/\n\u251c\u2500\u2500 0_search/            # Search Algorithms (Project 0)\n\u251c\u2500\u2500 1_tracking/          # Probabilistic Tracking (Project 1)\n\u251c\u2500\u2500 2_reinforcement/     # Reinforcement Learning (Project 2)\n\u2514\u2500\u2500 3_multiagent/        # Multi-Agent Systems (Project 3)\n</code></pre> <p>Each project directory contains: - Python source code implementations - Detailed README with algorithms and usage - DEVELOPMENT.md with implementation details - Test cases and autograder - Multiple layouts/environments for testing</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Each project has its own detailed documentation page (use navigation above). Generally:</p> <ol> <li>Prerequisites: Python 3.6+ with standard libraries</li> <li>Navigate: <code>cd 0_search</code> (or other project folder)</li> <li>Test: <code>python autograder.py</code> to validate implementations</li> <li>Run: Execute specific algorithms with command-line options</li> <li>Documentation: Click on project links above for comprehensive guides</li> </ol>"},{"location":"#contact","title":"Contact","text":"<p>Author: Praphul Samavedam   GitHub: @PraphulSamavedam</p>"},{"location":"multi-agent-systems/","title":"Multi-Agent Systems","text":""},{"location":"multi-agent-systems/#overview","title":"Overview","text":"<p>The Multi-Agent Systems project implements adversarial search algorithms for game playing where Pacman must make optimal decisions against intelligent ghost opponents. Built on game tree search principles, the system explores various decision-making strategies from reactive reflex agents to sophisticated minimax search with alpha-beta optimization and expectimax for probabilistic opponents.</p> <p>The implementation demonstrates how agents can reason about adversaries in competitive scenarios, achieving optimal play against perfect opponents through minimax while maintaining real-time performance (30ms per move) using alpha-beta pruning and carefully designed evaluation functions.</p>"},{"location":"multi-agent-systems/#system-architecture","title":"System Architecture","text":"<p>The multi-agent system follows a game tree search architecture with alternating agent layers:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    MULTI-AGENT PACMAN SYSTEM                                \u2502\n\u2502                         System Workflow Diagram                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     GAME ENVIRONMENT             \u2502\n                    \u2502  - Pacman (Player Agent)         \u2502\n                    \u2502  - Ghosts (Adversarial Agents)   \u2502\n                    \u2502  - Food &amp; Capsules               \u2502\n                    \u2502  - Maze Layout                   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     AGENT TYPES              \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                          \u2502                            \u2502\n        \u25bc                          \u25bc                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 REFLEX AGENT   \u2502      \u2502 MINIMAX AGENT  \u2502        \u2502 EXPECTIMAX     \u2502\n\u2502                \u2502      \u2502                \u2502        \u2502 AGENT          \u2502\n\u2502 - Immediate    \u2502      \u2502 - Perfect      \u2502        \u2502                \u2502\n\u2502   evaluation   \u2502      \u2502   adversary    \u2502        \u2502 - Probabilistic\u2502\n\u2502 - State-action \u2502      \u2502 - Min-Max      \u2502        \u2502   adversary    \u2502\n\u2502   pairs        \u2502      \u2502   search       \u2502        \u2502 - Expected     \u2502\n\u2502 - No lookahead \u2502      \u2502 - Optimal play \u2502        \u2502   values       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                         \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   ALPHA-BETA PRUNING    \u2502\n                    \u2502   - Optimization for    \u2502\n                    \u2502     Minimax search      \u2502\n                    \u2502   - Prunes branches     \u2502\n                    \u2502   - Same result as      \u2502\n                    \u2502     Minimax but faster  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  EVALUATION FUNCTION    \u2502\n                    \u2502  - Score game states    \u2502\n                    \u2502  - Heuristic features:  \u2502\n                    \u2502    * Distance to food   \u2502\n                    \u2502    * Ghost proximity    \u2502\n                    \u2502    * Scared ghosts      \u2502\n                    \u2502    * Food remaining     \u2502\n                    \u2502    * Game score         \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     GAME OUTPUT         \u2502\n                    \u2502  - Action selection     \u2502\n                    \u2502  - Visual display       \u2502\n                    \u2502  - Score tracking       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nKey Algorithms:\n\u251c\u2500 Reflex Agent: Immediate state evaluation\n\u251c\u2500 Minimax: Optimal adversarial search, O(b^d)\n\u251c\u2500 Alpha-Beta: Efficient minimax, O(b^(d/2)) best case\n\u2514\u2500 Expectimax: Probabilistic opponent modeling\n</code></pre>"},{"location":"multi-agent-systems/#features-and-capabilities","title":"Features and Capabilities","text":""},{"location":"multi-agent-systems/#agent-types","title":"Agent Types","text":""},{"location":"multi-agent-systems/#1-reflex-agent","title":"1. Reflex Agent","text":"<ul> <li>Strategy: Evaluate immediate successor states</li> <li>Lookahead: None (one-step evaluation)</li> <li>Computation: O(|A|) for |A| actions</li> <li>Response Time: &lt;1ms</li> <li>Best For: Fast decisions, simple scenarios</li> </ul>"},{"location":"multi-agent-systems/#2-minimax-agent","title":"2. Minimax Agent","text":"<ul> <li>Strategy: Adversarial search assuming optimal opponents</li> <li>Lookahead: Configurable depth (typically 2-4 plies)</li> <li>Computation: O(b^d) where b\u22484-5 branches, d=depth</li> <li>Response Time: 10-1000ms depending on depth</li> <li>Best For: Perfect opponents, optimal guaranteed solutions</li> </ul>"},{"location":"multi-agent-systems/#3-alpha-beta-agent","title":"3. Alpha-Beta Agent","text":"<ul> <li>Strategy: Optimized minimax with branch pruning</li> <li>Lookahead: Same as minimax, but deeper possible</li> <li>Computation: O(b^(d/2)) to O(b^d) depending on ordering</li> <li>Response Time: 2-10x faster than minimax</li> <li>Best For: When minimax needed but faster execution required</li> </ul>"},{"location":"multi-agent-systems/#4-expectimax-agent","title":"4. Expectimax Agent","text":"<ul> <li>Strategy: Models random/suboptimal opponents</li> <li>Lookahead: Configurable depth</li> <li>Computation: O(b^d) (no pruning possible)</li> <li>Response Time: Similar to minimax</li> <li>Best For: Random or probabilistic opponents</li> </ul>"},{"location":"multi-agent-systems/#quick-start","title":"Quick Start","text":""},{"location":"multi-agent-systems/#installation","title":"Installation","text":"<p>Navigate to project directory: <pre><code>cd 3_multiagent\n</code></pre></p>"},{"location":"multi-agent-systems/#running-agents","title":"Running Agents","text":""},{"location":"multi-agent-systems/#reflex-agent","title":"Reflex Agent","text":"<pre><code># Test on classic layout\npython pacman.py -p ReflexAgent -l testClassic\n\n# Multiple games for statistics\npython pacman.py -p ReflexAgent -l testClassic -n 10\n</code></pre>"},{"location":"multi-agent-systems/#minimax-agent","title":"Minimax Agent","text":"<pre><code># Run on minimax-designed layout\npython pacman.py -p MinimaxAgent -l minimaxClassic -a depth=4\n\n# Medium classic with depth 3\npython pacman.py -p MinimaxAgent -l mediumClassic -a depth=3\n</code></pre>"},{"location":"multi-agent-systems/#alpha-beta-agent","title":"Alpha-Beta Agent","text":"<pre><code># Optimized search\npython pacman.py -p AlphaBetaAgent -l smallClassic -a depth=3\n\n# Fast mode (no graphics)\npython pacman.py -p AlphaBetaAgent -l mediumClassic -a depth=4 -q -n 10\n</code></pre>"},{"location":"multi-agent-systems/#expectimax-agent","title":"Expectimax Agent","text":"<pre><code># Model random ghosts\npython pacman.py -p ExpectimaxAgent -l minimaxClassic -a depth=3\n\n# Compare with minimax\npython pacman.py -p MinimaxAgent -l minimaxClassic -a depth=3\npython pacman.py -p ExpectimaxAgent -l minimaxClassic -a depth=3\n</code></pre>"},{"location":"multi-agent-systems/#command-line-options","title":"Command Line Options","text":"Option Description Values <code>-p</code> Agent type <code>ReflexAgent</code>, <code>MinimaxAgent</code>, <code>AlphaBetaAgent</code>, <code>ExpectimaxAgent</code> <code>-l</code> Layout/maze <code>testClassic</code>, <code>minimaxClassic</code>, <code>smallClassic</code>, <code>mediumClassic</code> <code>-a</code> Agent args <code>depth=N</code>, <code>evalFn=better</code> <code>-q</code> Quiet mode No graphics, faster <code>-n</code> Games to play <code>1</code>, <code>10</code>, <code>100</code> <code>--frameTime</code> Animation <code>0</code> (fast), <code>0.1</code> (slow)"},{"location":"multi-agent-systems/#algorithm-details","title":"Algorithm Details","text":""},{"location":"multi-agent-systems/#minimax-search","title":"Minimax Search","text":"<p>Core Idea: Assume all agents play optimally</p> <p>Pseudocode: <pre><code>function MINIMAX(state, depth, agentIndex):\n    if depth = 0 or state is terminal:\n        return EVALUATE(state)\n\n    if agentIndex = 0:  # Pacman (MAX)\n        value = -\u221e\n        for each action in legal_actions:\n            successor = generate_successor(state, action)\n            value = max(value, MINIMAX(successor, depth, 1))\n        return value\n\n    else:  # Ghost (MIN)\n        value = +\u221e\n        for each action in legal_actions:\n            successor = generate_successor(state, action)\n            nextAgent = (agentIndex + 1) % numAgents\n            nextDepth = depth - 1 if nextAgent = 0 else depth\n            value = min(value, MINIMAX(successor, nextDepth, nextAgent))\n        return value\n</code></pre></p> <p>Implementation: <pre><code>def minimax(self, gameState, depth, agentIndex):\n    # Terminal conditions\n    if gameState.isWin() or gameState.isLose() or depth == 0:\n        return self.evaluationFunction(gameState)\n\n    numAgents = gameState.getNumAgents()\n    legalActions = gameState.getLegalActions(agentIndex)\n\n    # Pacman's turn (MAX)\n    if agentIndex == 0:\n        return max(self.minimax(gameState.generateSuccessor(agentIndex, action),\n                               depth, 1)\n                  for action in legalActions)\n\n    # Ghost's turn (MIN)\n    else:\n        nextAgent = (agentIndex + 1) % numAgents\n        nextDepth = depth - 1 if nextAgent == 0 else depth\n\n        return min(self.minimax(gameState.generateSuccessor(agentIndex, action),\n                               nextDepth, nextAgent)\n                  for action in legalActions)\n</code></pre></p> <p>Properties: - Optimal against perfect opponents - Explores full game tree to depth d - Exponential time complexity O(b^d) - Guarantees best worst-case outcome</p>"},{"location":"multi-agent-systems/#alpha-beta-pruning","title":"Alpha-Beta Pruning","text":"<p>Core Idea: Prune branches that won't affect decision</p> <p>Parameters: - \u03b1 (alpha): Best value MAX can guarantee (lower bound) - \u03b2 (beta): Best value MIN can guarantee (upper bound)</p> <p>Pruning Rule: <pre><code>If \u03b1 \u2265 \u03b2 \u2192 prune remaining branches\n</code></pre></p> <p>Pseudocode: <pre><code>function ALPHA-BETA(state, depth, agentIndex, \u03b1, \u03b2):\n    if depth = 0 or terminal:\n        return EVALUATE(state)\n\n    if agentIndex = 0:  # MAX\n        value = -\u221e\n        for each action:\n            value = max(value, ALPHA-BETA(successor, depth, 1, \u03b1, \u03b2))\n            \u03b1 = max(\u03b1, value)\n            if \u03b1 \u2265 \u03b2:\n                break  # \u03b2 cutoff\n        return value\n\n    else:  # MIN\n        value = +\u221e\n        for each action:\n            value = min(value, ALPHA-BETA(successor, nextDepth, nextAgent, \u03b1, \u03b2))\n            \u03b2 = min(\u03b2, value)\n            if \u03b1 \u2265 \u03b2:\n                break  # \u03b1 cutoff\n        return value\n</code></pre></p> <p>Implementation: <pre><code>def alphaBeta(self, gameState, depth, agentIndex, alpha, beta):\n    if gameState.isWin() or gameState.isLose() or depth == 0:\n        return self.evaluationFunction(gameState)\n\n    legalActions = gameState.getLegalActions(agentIndex)\n\n    # MAX node\n    if agentIndex == 0:\n        value = float('-inf')\n        for action in legalActions:\n            successor = gameState.generateSuccessor(agentIndex, action)\n            value = max(value, self.alphaBeta(successor, depth, 1, alpha, beta))\n\n            # Pruning\n            if value &gt; beta:\n                return value\n            alpha = max(alpha, value)\n        return value\n\n    # MIN node\n    else:\n        value = float('inf')\n        nextAgent = (agentIndex + 1) % gameState.getNumAgents()\n        nextDepth = depth - 1 if nextAgent == 0 else depth\n\n        for action in legalActions:\n            successor = gameState.generateSuccessor(agentIndex, action)\n            value = min(value, self.alphaBeta(successor, nextDepth, nextAgent, alpha, beta))\n\n            # Pruning\n            if value &lt; alpha:\n                return value\n            beta = min(beta, value)\n        return value\n</code></pre></p> <p>Efficiency: - Best case: O(b^(d/2)) - perfect move ordering - Average case: O(b^(3d/4)) - Typical speedup: 3-5x over minimax - Enables search 1-2 plies deeper</p>"},{"location":"multi-agent-systems/#expectimax-search","title":"Expectimax Search","text":"<p>Core Idea: Model probabilistic opponents with expected values</p> <p>Pseudocode: <pre><code>function EXPECTIMAX(state, depth, agentIndex):\n    if depth = 0 or terminal:\n        return EVALUATE(state)\n\n    if agentIndex = 0:  # MAX (Pacman)\n        return max_{action} EXPECTIMAX(successor, depth, 1)\n\n    else:  # CHANCE (Ghost)\n        actions = legal_actions\n        probability = 1.0 / len(actions)\n        total = 0\n\n        for each action:\n            total += probability \u00d7 EXPECTIMAX(successor, nextDepth, nextAgent)\n\n        return total\n</code></pre></p> <p>Implementation: <pre><code>def expectimax(self, gameState, depth, agentIndex):\n    if gameState.isWin() or gameState.isLose() or depth == 0:\n        return self.evaluationFunction(gameState)\n\n    legalActions = gameState.getLegalActions(agentIndex)\n\n    # MAX node (Pacman)\n    if agentIndex == 0:\n        return max(self.expectimax(gameState.generateSuccessor(agentIndex, action),\n                                  depth, 1)\n                  for action in legalActions)\n\n    # CHANCE node (Ghost)\n    else:\n        probability = 1.0 / len(legalActions)\n        nextAgent = (agentIndex + 1) % gameState.getNumAgents()\n        nextDepth = depth - 1 if nextAgent == 0 else depth\n\n        return sum(probability * self.expectimax(\n                       gameState.generateSuccessor(agentIndex, action),\n                       nextDepth, nextAgent)\n                  for action in legalActions)\n</code></pre></p> <p>Properties: - More realistic than minimax (ghosts not optimal) - Uses expected values instead of worst case - Cannot be pruned (must explore all branches) - Better for random opponents</p>"},{"location":"multi-agent-systems/#evaluation-functions","title":"Evaluation Functions","text":""},{"location":"multi-agent-systems/#design-principles","title":"Design Principles","text":"<p>Goal: Score game states where higher = better for Pacman</p> <p>Components: 1. Game score: Current score as baseline 2. Food distance: Closer to food = higher score 3. Ghost distance: Farther from ghosts = higher score (unless scared) 4. Scared ghosts: Closer to scared ghosts = higher score 5. Food remaining: Fewer food = higher score</p>"},{"location":"multi-agent-systems/#example-implementation","title":"Example Implementation","text":"<pre><code>def betterEvaluationFunction(currentGameState):\n    \"\"\"\n    Advanced evaluation function\n    \"\"\"\n    # Terminal states\n    if currentGameState.isWin():\n        return float('inf')\n    if currentGameState.isLose():\n        return float('-inf')\n\n    # Extract state information\n    pos = currentGameState.getPacmanPosition()\n    food = currentGameState.getFood()\n    ghosts = currentGameState.getGhostStates()\n    capsules = currentGameState.getCapsules()\n    score = currentGameState.getScore()\n\n    # Feature 1: Food distance\n    foodList = food.asList()\n    foodScore = 0\n    if foodList:\n        minFoodDist = min([manhattanDistance(pos, f) for f in foodList])\n        foodScore = 10.0 / (minFoodDist + 1)\n\n    # Feature 2: Ghost distance\n    ghostScore = 0\n    for ghost in ghosts:\n        ghostPos = ghost.getPosition()\n        ghostDist = manhattanDistance(pos, ghostPos)\n\n        if ghost.scaredTimer &gt; 0:\n            # Chase scared ghosts\n            ghostScore += 200.0 / (ghostDist + 1)\n        else:\n            # Avoid active ghosts\n            if ghostDist &lt; 2:\n                ghostScore -= 1000\n            else:\n                ghostScore += ghostDist * 2\n\n    # Feature 3: Capsules\n    capsuleScore = 0\n    if capsules:\n        minCapsuleDist = min([manhattanDistance(pos, c) for c in capsules])\n        capsuleScore = 50.0 / (minCapsuleDist + 1)\n\n    # Feature 4: Food remaining (penalty)\n    foodRemainingScore = -5 * len(foodList)\n\n    # Combine features with weights\n    totalScore = (score +\n                  foodScore +\n                  ghostScore +\n                  capsuleScore +\n                  foodRemainingScore)\n\n    return totalScore\n</code></pre>"},{"location":"multi-agent-systems/#feature-design-tips","title":"Feature Design Tips","text":"<ol> <li> <p>Use reciprocal for attraction: <pre><code>score += weight / (distance + 1)  # Avoid division by zero\n</code></pre></p> </li> <li> <p>Use direct distance for repulsion: <pre><code>score += weight * distance  # or -weight / distance\n</code></pre></p> </li> <li> <p>Scale features appropriately: <pre><code># Normalize by maze size\nnormalizedDist = distance / (width + height)\n</code></pre></p> </li> <li> <p>Handle edge cases: <pre><code># Empty food list\nif not foodList:\n    return score  # Don't crash\n\n# Terminal states\nif gameState.isWin():\n    return float('inf')\n</code></pre></p> </li> </ol>"},{"location":"multi-agent-systems/#performance-analysis","title":"Performance Analysis","text":""},{"location":"multi-agent-systems/#algorithm-comparison","title":"Algorithm Comparison","text":"<p>Small Classic Layout (depth=3):</p> Agent Avg Score Win Rate Time/Move Nodes Expanded Reflex 1200 85% &lt;1ms N/A Minimax 1400 95% 150ms ~500 Alpha-Beta 1400 95% 50ms ~180 Expectimax 1350 90% 200ms ~500 <p>Medium Classic Layout (depth=3):</p> Agent Avg Score Win Rate Time/Move Nodes Expanded Minimax 1800 85% 800ms ~2500 Alpha-Beta 1800 85% 250ms ~900 Expectimax 1750 80% 900ms ~2500 <p>Key Observations: - Alpha-Beta achieves same decisions as Minimax with 3-5x speedup - Expectimax better models random ghosts but can't be pruned - Deeper search improves decisions but increases computation exponentially</p>"},{"location":"multi-agent-systems/#depth-vs-performance","title":"Depth vs Performance","text":"<p>Minimax Agent on mediumClassic:</p> Depth Avg Score Win Rate Time/Move Nodes Expanded 2 1600 75% 80ms ~150 3 1800 85% 800ms ~2500 4 1900 90% 8s ~40000 5 1950 95% 120s ~600000 <p>Practical Depth Limits: - Depth 2-3: Playable in real-time - Depth 4: Slow but acceptable - Depth 5+: Impractical without optimizations</p>"},{"location":"multi-agent-systems/#usage-examples","title":"Usage Examples","text":""},{"location":"multi-agent-systems/#basic-game-playing","title":"Basic Game Playing","text":"<pre><code># Reflex agent (fast, reactive)\npython pacman.py -p ReflexAgent -l testClassic\n\n# Minimax (optimal but slow)\npython pacman.py -p MinimaxAgent -l minimaxClassic -a depth=4\n\n# Alpha-beta (optimal and fast)\npython pacman.py -p AlphaBetaAgent -l smallClassic -a depth=3\n\n# Expectimax (probabilistic)\npython pacman.py -p ExpectimaxAgent -l minimaxClassic -a depth=3\n</code></pre>"},{"location":"multi-agent-systems/#performance-testing","title":"Performance Testing","text":"<pre><code># Maximum speed comparison\npython pacman.py -p MinimaxAgent -l smallClassic -a depth=3 -q -n 10\npython pacman.py -p AlphaBetaAgent -l smallClassic -a depth=3 -q -n 10\n\n# Depth comparison\nfor d in 2 3 4; do\n    echo \"Testing depth $d\"\n    python pacman.py -p AlphaBetaAgent -l smallClassic -a depth=$d -q -n 5\ndone\n</code></pre>"},{"location":"multi-agent-systems/#evaluation-function-testing","title":"Evaluation Function Testing","text":"<pre><code># Test custom evaluation\npython pacman.py -p AlphaBetaAgent -a depth=2,evalFn=better -l smallClassic\n\n# Compare default vs custom\npython pacman.py -p AlphaBetaAgent -a depth=2 -l mediumClassic -n 10 -q\npython pacman.py -p AlphaBetaAgent -a depth=2,evalFn=better -l mediumClassic -n 10 -q\n</code></pre>"},{"location":"multi-agent-systems/#key-concepts","title":"Key Concepts","text":""},{"location":"multi-agent-systems/#game-tree-structure","title":"Game Tree Structure","text":"<p>Alternating Layers: <pre><code>Depth 0: Pacman (MAX)\n         |\nDepth 1: Ghost1 (MIN)\n         |\nDepth 2: Ghost2 (MIN)\n         |\nDepth 3: Pacman (MAX)\n         ...\n</code></pre></p> <p>Ply vs Depth: - Ply: Single agent move - Depth: Complete round (all agents moved) - Depth d: d \u00d7 numAgents plies</p>"},{"location":"multi-agent-systems/#minimax-value","title":"Minimax Value","text":"<p>Recursive Definition: <pre><code>MiniMax(s, 0) = Eval(s)  # depth 0\n\nMiniMax(s, d, Pacman) = max_{a} MiniMax(successor(s,a), d, Ghost1)\n\nMiniMax(s, d, Ghost_i) = min_{a} MiniMax(successor(s,a), d, Ghost_{i+1})\n\nMiniMax(s, d, Ghost_last) = min_{a} MiniMax(successor(s,a), d-1, Pacman)\n</code></pre></p>"},{"location":"multi-agent-systems/#alpha-beta-pruning_1","title":"Alpha-Beta Pruning","text":"<p>Alpha (\u03b1): Best value MAX can guarantee so far Beta (\u03b2): Best value MIN can guarantee so far</p> <p>Pruning Conditions: - In MAX node: If value \u2265 \u03b2, prune (MIN won't choose this path) - In MIN node: If value \u2264 \u03b1, prune (MAX won't choose this path)</p> <p>Update Rules: - MAX node: \u03b1 = max(\u03b1, value) - MIN node: \u03b2 = min(\u03b2, value)</p> <p>Example: <pre><code>Root (MAX, \u03b1=-\u221e, \u03b2=+\u221e)\n\u251c\u2500 Action1 \u2192 MIN (\u03b1=-\u221e, \u03b2=+\u221e)\n\u2502  \u251c\u2500 Child1 \u2192 value = 5\n\u2502  \u2502  Update \u03b2 = min(+\u221e, 5) = 5\n\u2502  \u2514\u2500 Child2 \u2192 value = 3\n\u2502     Update \u03b2 = min(5, 3) = 3\n\u2502     Return 3 to MAX\n\u2502  Update \u03b1 = max(-\u221e, 3) = 3\n\u2502\n\u251c\u2500 Action2 \u2192 MIN (\u03b1=3, \u03b2=+\u221e)\n\u2502  \u251c\u2500 Child1 \u2192 value = 2\n\u2502     Since 2 \u2264 \u03b1=3, PRUNE! Return 2\n\u2502  Update \u03b1 = max(3, 2) = 3 (no change)\n\u2502\n\u2514\u2500 Action3 \u2192 MIN (\u03b1=3, \u03b2=+\u221e)\n   ... continue ...\n\nBest Action = Action1 (value=3)\n</code></pre></p>"},{"location":"multi-agent-systems/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"multi-agent-systems/#automated-testing","title":"Automated Testing","text":"<pre><code># All questions\npython autograder.py\n\n# Individual questions\npython autograder.py -q q1  # Reflex Agent\npython autograder.py -q q2  # Minimax\npython autograder.py -q q3  # Alpha-Beta\npython autograder.py -q q4  # Expectimax\npython autograder.py -q q5  # Evaluation Function\n\n# Verbose debugging\npython autograder.py -q q2 --verbose\n</code></pre>"},{"location":"multi-agent-systems/#manual-testing","title":"Manual Testing","text":"<pre><code># Visual testing\npython pacman.py -p MinimaxAgent -l testClassic -a depth=2\n\n# Performance comparison\npython pacman.py -p MinimaxAgent -l smallClassic -a depth=3 -n 10 -q\npython pacman.py -p AlphaBetaAgent -l smallClassic -a depth=3 -n 10 -q\n\n# Evaluation function impact\npython pacman.py -p AlphaBetaAgent -a depth=2 -l mediumClassic -n 20 -q\npython pacman.py -p AlphaBetaAgent -a depth=2,evalFn=better -l mediumClassic -n 20 -q\n</code></pre>"},{"location":"multi-agent-systems/#troubleshooting","title":"Troubleshooting","text":""},{"location":"multi-agent-systems/#agent-too-slow","title":"Agent Too Slow","text":"<p>Problem: Takes too long per move</p> <p>Solutions: - Reduce search depth (try depth=2) - Optimize evaluation function - Use Alpha-Beta instead of Minimax - Profile code for bottlenecks - Consider iterative deepening</p>"},{"location":"multi-agent-systems/#agent-makes-poor-decisions","title":"Agent Makes Poor Decisions","text":"<p>Problem: Loses frequently or makes obvious mistakes</p> <p>Solutions: - Improve evaluation function (add features) - Increase search depth (if time permits) - Tune feature weights - Check terminal state handling - Verify minimax logic is correct</p>"},{"location":"multi-agent-systems/#alpha-beta-different-from-minimax","title":"Alpha-Beta Different from Minimax","text":"<p>Problem: Different actions selected</p> <p>Solutions: - Check pruning conditions (\u03b1 \u2265 \u03b2) - Verify \u03b1/\u03b2 updates in correct nodes - Ensure return value correct after pruning - Test on simple cases first - Compare intermediate values</p>"},{"location":"multi-agent-systems/#expectimax-always-loses","title":"Expectimax Always Loses","text":"<p>Problem: Poor win rate</p> <p>Solutions: - Verify uniform probability distribution - Check expected value calculation (sum, not min) - Ensure correct agent turn handling - Improve evaluation function - Increase search depth</p>"},{"location":"multi-agent-systems/#learning-objectives","title":"Learning Objectives","text":"<p>\u2705 Adversarial Search - Understand zero-sum games and minimax principle - Implement minimax algorithm with alternating agents - Handle multi-agent turns correctly - Extract optimal actions from game tree</p> <p>\u2705 Alpha-Beta Pruning - Understand pruning conditions and invariants - Implement efficient minimax optimization - Manage alpha and beta bounds correctly - Analyze pruning effectiveness</p> <p>\u2705 Expectimax Search - Model probabilistic opponent behavior - Compute expected values over chance nodes - Understand when minimax vs expectimax is appropriate - Handle uncertainty in opponent actions</p> <p>\u2705 Evaluation Functions - Design heuristics for non-terminal states - Balance multiple competing objectives - Normalize and weight features appropriately - Create fast and informative evaluations</p>"},{"location":"multi-agent-systems/#additional-resources","title":"Additional Resources","text":""},{"location":"multi-agent-systems/#gamestate-methods","title":"GameState Methods","text":"<pre><code>gameState.getLegalActions(agentIndex)          # Valid actions for agent\ngameState.generateSuccessor(agentIndex, action) # Result of taking action\ngameState.getNumAgents()                       # Total agents (Pacman + ghosts)\ngameState.getPacmanPosition()                  # (x, y) position\ngameState.getGhostPositions()                  # List of ghost positions\ngameState.getFood()                            # Food grid\ngameState.getCapsules()                        # Power pellet locations\ngameState.getScore()                           # Current game score\ngameState.isWin()                              # Victory check\ngameState.isLose()                             # Defeat check\n</code></pre>"},{"location":"multi-agent-systems/#utility-functions","title":"Utility Functions","text":"<pre><code>from util import manhattanDistance              # L1 distance\n\n# Game directions\nfrom game import Directions\nDirections.NORTH, SOUTH, EAST, WEST, STOP\n</code></pre>"},{"location":"multi-agent-systems/#references","title":"References","text":"<ul> <li>UC Berkeley CS188 - Original project framework</li> <li>Russell &amp; Norvig - \"Artificial Intelligence: A Modern Approach\" (Chapters 5-6)</li> <li>Game Theory Literature - Minimax theorem, zero-sum games</li> <li>Course Materials - CS-5100 lecture slides on adversarial search</li> </ul>"},{"location":"probabilistic-tracking/","title":"Probabilistic Tracking","text":""},{"location":"probabilistic-tracking/#overview","title":"Overview","text":"<p>The Probabilistic Tracking project implements Bayesian inference and Hidden Markov Model (HMM) algorithms for tracking hidden ghost positions using noisy sensor observations. The system demonstrates principled reasoning under uncertainty through exact inference (forward algorithm) and approximate inference (particle filtering) to maintain probability distributions over ghost locations.</p> <p>The implementation handles single and multiple target tracking scenarios, achieving real-time performance (30+ FPS) while maintaining accurate belief distributions despite sensor noise. The system showcases how probabilistic methods enable robust tracking in uncertain environments.</p>"},{"location":"probabilistic-tracking/#system-architecture","title":"System Architecture","text":"<p>The tracking system follows a probabilistic inference pipeline with belief propagation:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  PROBABILISTIC TRACKING SYSTEM                              \u2502\n\u2502                       System Workflow Diagram                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   HIDDEN MARKOV MODEL            \u2502\n                    \u2502  - Hidden States (Ghost Position)\u2502\n                    \u2502  - Observations (Noisy Distances)\u2502\n                    \u2502  - Transition Model P(X_t|X_t-1) \u2502\n                    \u2502  - Sensor Model P(e_t|X_t)       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                          \u2502                            \u2502\n        \u25bc                          \u25bc                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 EXACT          \u2502      \u2502 PARTICLE       \u2502        \u2502 JOINT          \u2502\n\u2502 INFERENCE      \u2502      \u2502 FILTERING      \u2502        \u2502 PARTICLE       \u2502\n\u2502                \u2502      \u2502                \u2502        \u2502 FILTERING      \u2502\n\u2502 - Forward      \u2502      \u2502 - Sampling     \u2502        \u2502                \u2502\n\u2502   algorithm    \u2502      \u2502 - Resampling   \u2502        \u2502 - Multiple     \u2502\n\u2502 - Belief state \u2502      \u2502 - Approximate  \u2502        \u2502   ghosts       \u2502\n\u2502 - Exact        \u2502      \u2502   tracking     \u2502        \u2502 - Joint dist.  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                         \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   BELIEF UPDATE         \u2502\n                    \u2502  P(X_t|e_1:t) =         \u2502\n                    \u2502    \u03b1 P(e_t|X_t)         \u2502\n                    \u2502    \u03a3 P(X_t|x_t-1)       \u2502\n                    \u2502      P(x_t-1|e_1:t-1)   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   GHOST HUNTING         \u2502\n                    \u2502  - Choose actions       \u2502\n                    \u2502  - Move toward beliefs  \u2502\n                    \u2502  - Bust ghosts          \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nKey Algorithms:\n\u251c\u2500 Exact Inference: Forward algorithm, O(|X|\u00b2) per step\n\u251c\u2500 Particle Filter: Sampling-based, O(N) for N particles\n\u2514\u2500 Joint Particle: Multi-target tracking, O(N\u00d7M) for M ghosts\n</code></pre>"},{"location":"probabilistic-tracking/#features-and-capabilities","title":"Features and Capabilities","text":""},{"location":"probabilistic-tracking/#inference-methods","title":"Inference Methods","text":""},{"location":"probabilistic-tracking/#1-bayesian-network-construction","title":"1. Bayesian Network Construction","text":"<ul> <li>Purpose: Represent conditional dependencies between variables</li> <li>Structure: Pacman \u2192 Observations \u2190 Ghosts</li> <li>Components: Variables, edges, conditional probability tables (CPTs)</li> <li>Use Case: Modeling sensor dependencies</li> </ul>"},{"location":"probabilistic-tracking/#2-exact-inference","title":"2. Exact Inference","text":"<ul> <li>Method: Forward algorithm (recursive belief update)</li> <li>Accuracy: Exact probability distribution</li> <li>Complexity: O(|X|\u00b2) per time step</li> <li>Memory: O(|X|) for belief distribution</li> <li>Best For: Small state spaces (~1000 positions)</li> </ul>"},{"location":"probabilistic-tracking/#3-particle-filtering","title":"3. Particle Filtering","text":"<ul> <li>Method: Sampling-based approximation</li> <li>Accuracy: Approximates true distribution with N particles</li> <li>Complexity: O(N) per time step</li> <li>Memory: O(N) for N particles</li> <li>Best For: Large state spaces, real-time requirements</li> </ul>"},{"location":"probabilistic-tracking/#4-joint-particle-filtering","title":"4. Joint Particle Filtering","text":"<ul> <li>Method: Sample joint distributions over all ghosts</li> <li>Accuracy: Approximate joint beliefs</li> <li>Complexity: O(N \u00d7 M) for M ghosts</li> <li>Memory: O(N) particles, each with M positions</li> <li>Best For: Multiple target tracking with interactions</li> </ul>"},{"location":"probabilistic-tracking/#tracking-scenarios","title":"Tracking Scenarios","text":""},{"location":"probabilistic-tracking/#single-ghost-tracking","title":"Single Ghost Tracking","text":"<ul> <li>Track one ghost with exact or approximate inference</li> <li>Handle noisy distance observations</li> <li>Update beliefs in real-time</li> </ul>"},{"location":"probabilistic-tracking/#multiple-ghost-tracking","title":"Multiple Ghost Tracking","text":"<ul> <li>Track 2-4 ghosts simultaneously</li> <li>Maintain joint probability distributions</li> <li>Handle conditional independence assumptions</li> </ul>"},{"location":"probabilistic-tracking/#quick-start","title":"Quick Start","text":""},{"location":"probabilistic-tracking/#installation","title":"Installation","text":"<p>Ensure Python 3.6+ is installed: <pre><code>python --version\n</code></pre></p> <p>Navigate to project directory: <pre><code>cd 1_tracking\n</code></pre></p>"},{"location":"probabilistic-tracking/#running-tracking-agents","title":"Running Tracking Agents","text":""},{"location":"probabilistic-tracking/#exact-inference-single-ghost","title":"Exact Inference (Single Ghost)","text":"<pre><code>python busters.py -p BasicAgentAA -l trickyClassic -k 1\n</code></pre>"},{"location":"probabilistic-tracking/#particle-filtering-single-ghost","title":"Particle Filtering (Single Ghost)","text":"<pre><code>python busters.py -p ParticleAgent -l trickyClassic -k 1\n</code></pre>"},{"location":"probabilistic-tracking/#joint-particle-filtering-multiple-ghosts","title":"Joint Particle Filtering (Multiple Ghosts)","text":"<pre><code>python busters.py -p JointParticleAgent -l trickyClassic -k 2\n</code></pre>"},{"location":"probabilistic-tracking/#command-line-options","title":"Command Line Options","text":"Option Description Example <code>-p</code> Agent type <code>BasicAgentAA</code>, <code>ParticleAgent</code>, <code>JointParticleAgent</code> <code>-l</code> Layout/map <code>trickyClassic</code>, <code>smallClassic</code> <code>-k</code> Number of ghosts <code>1</code>, <code>2</code>, <code>3</code> <code>-n</code> Number of games <code>10</code> <code>-q</code> Quiet mode No graphics <code>--frameTime</code> Animation speed <code>0</code> (fast), <code>0.1</code> (slow)"},{"location":"probabilistic-tracking/#algorithm-details","title":"Algorithm Details","text":""},{"location":"probabilistic-tracking/#bayesian-network-structure","title":"Bayesian Network Structure","text":"<p>Variables: - Pacman: Position (observable) - Ghost0, Ghost1: Hidden positions - Obs0, Obs1: Noisy distance observations</p> <p>Dependencies: <pre><code>      Pacman\n     /      \\\n    /        \\\nGhost0      Ghost1\n    |          |\n    |          |\n  Obs0       Obs1\n</code></pre></p> <p>Conditional Probability: <pre><code>P(Obs_i | Ghost_i, Pacman) = sensor model\nP(Ghost_t | Ghost_{t-1}) = transition model\n</code></pre></p>"},{"location":"probabilistic-tracking/#exact-inference-forward-algorithm","title":"Exact Inference (Forward Algorithm)","text":"<p>Belief Update Formula: <pre><code>P(X_t | e_{1:t}) = \u03b1 P(e_t | X_t) \u03a3_{x_{t-1}} P(X_t | x_{t-1}) P(x_{t-1} | e_{1:t-1})\n</code></pre></p> <p>Algorithm Steps:</p> <ol> <li> <p>Initialize: Uniform distribution over all positions <pre><code>for pos in legalPositions:\n    beliefs[pos] = 1.0 / len(legalPositions)\n</code></pre></p> </li> <li> <p>Observe Update: Apply Bayes rule <pre><code>for pos in legalPositions:\n    # P(e | X=pos)\n    observationProb = getObservationProb(observation, pacmanPos, pos)\n\n    # P(X=pos | e) \u221d P(e | X=pos) \u00d7 P(X=pos)\n    beliefs[pos] *= observationProb\n\n# Normalize to probability distribution\nbeliefs.normalize()\n</code></pre></p> </li> <li> <p>Time Elapse: Predict next position <pre><code>newBeliefs = Counter()\n\nfor oldPos in legalPositions:\n    # P(newPos | oldPos)\n    transitionDist = getPositionDistribution(oldPos)\n\n    for newPos, prob in transitionDist.items():\n        # \u03a3 P(X_t | x_{t-1}) P(x_{t-1})\n        newBeliefs[newPos] += prob * beliefs[oldPos]\n\nbeliefs = newBeliefs\nbeliefs.normalize()\n</code></pre></p> </li> </ol> <p>Properties: - Exact probability distribution - Guaranteed correct with perfect models - Computationally expensive for large spaces</p>"},{"location":"probabilistic-tracking/#particle-filtering","title":"Particle Filtering","text":"<p>Core Idea: Represent belief with N samples (particles)</p> <p>Algorithm Steps:</p> <ol> <li> <p>Initialize: Sample N particles uniformly <pre><code>particles = []\nfor i in range(N):\n    particles.append(random.choice(legalPositions))\n</code></pre></p> </li> <li> <p>Time Elapse: Move each particle <pre><code>newParticles = []\nfor particle in particles:\n    # Sample from transition model\n    newPosDist = getPositionDistribution(particle)\n    newParticle = sample(newPosDist)\n    newParticles.append(newParticle)\n</code></pre></p> </li> <li> <p>Observe Update: Weight and resample <pre><code>weights = []\nfor particle in particles:\n    # Weight by observation likelihood\n    weight = getObservationProb(observation, pacmanPos, particle)\n    weights.append(weight)\n\n# Resample with replacement\nparticles = weightedSample(particles, weights, N)\n</code></pre></p> </li> <li> <p>Extract Belief: <pre><code>beliefs = Counter()\nfor particle in particles:\n    beliefs[particle] += 1.0 / N\n</code></pre></p> </li> </ol> <p>Properties: - Approximate but scalable - O(N) complexity per update - Quality improves with more particles</p>"},{"location":"probabilistic-tracking/#joint-particle-filtering","title":"Joint Particle Filtering","text":"<p>Joint State Representation: <pre><code># Each particle is tuple of all ghost positions\nparticle = (ghost0_pos, ghost1_pos, ghost2_pos, ...)\n</code></pre></p> <p>Joint Update: <pre><code># Weight by product of observation likelihoods\nfor particle in particles:\n    weight = 1.0\n    for i in range(numGhosts):\n        ghostPos = particle[i]\n        obs = observations[i]\n        weight *= getObservationProb(obs, pacmanPos, ghostPos, i)\n    weights.append(weight)\n\n# Resample joint particles\nparticles = weightedSample(particles, weights, N)\n</code></pre></p> <p>Properties: - Tracks correlations between ghosts - Exponentially harder with more ghosts - Requires more particles than single-target</p>"},{"location":"probabilistic-tracking/#performance-analysis","title":"Performance Analysis","text":""},{"location":"probabilistic-tracking/#exact-inference","title":"Exact Inference","text":"State Space Size Update Time Memory Accuracy 100 positions ~1ms 1KB Exact 500 positions ~25ms 5KB Exact 1000 positions ~100ms 10KB Exact <p>Scalability Limit: ~1000-2000 positions for real-time</p>"},{"location":"probabilistic-tracking/#particle-filtering_1","title":"Particle Filtering","text":"Particle Count Update Time Memory Accuracy 100 particles ~2ms &lt;1KB ~85% 500 particles ~10ms ~5KB ~95% 1000 particles ~20ms ~10KB ~98% <p>Recommended: 200-500 particles for good balance</p>"},{"location":"probabilistic-tracking/#joint-particle-filtering_1","title":"Joint Particle Filtering","text":"Ghosts Particles Update Time Accuracy 2 ghosts 500 ~15ms ~90% 3 ghosts 1000 ~40ms ~85% 4 ghosts 2000 ~100ms ~80% <p>Note: Requires exponentially more particles with more ghosts</p>"},{"location":"probabilistic-tracking/#usage-examples","title":"Usage Examples","text":""},{"location":"probabilistic-tracking/#basic-ghost-tracking","title":"Basic Ghost Tracking","text":"<pre><code># Exact inference on single ghost\npython busters.py -p BasicAgentAA -l trickyClassic -k 1\n\n# Particle filtering (200 particles)\npython busters.py -p ParticleAgent -l trickyClassic -k 1\n\n# Visual comparison\npython busters.py -p BasicAgentAA -l smallClassic --frameTime=0.1\npython busters.py -p ParticleAgent -l smallClassic --frameTime=0.1\n</code></pre>"},{"location":"probabilistic-tracking/#multiple-ghost-scenarios","title":"Multiple Ghost Scenarios","text":"<pre><code># Track 2 ghosts with joint particles\npython busters.py -p JointParticleAgent -l trickyClassic -k 2\n\n# Track 3 ghosts (more challenging)\npython busters.py -p JointParticleAgent -l trickyClassic -k 3\n\n# Performance test\npython busters.py -p JointParticleAgent -k 2 -n 10 -q\n</code></pre>"},{"location":"probabilistic-tracking/#testing-and-evaluation","title":"Testing and Evaluation","text":"<pre><code># Run autograder\npython autograder.py\n\n# Test specific questions\npython autograder.py -q q1  # Bayes Net\npython autograder.py -q q2  # Exact Observe\npython autograder.py -q q3  # Exact Elapse\npython autograder.py -q q4  # Particle Filter\npython autograder.py -q q5  # Joint Particle\n</code></pre>"},{"location":"probabilistic-tracking/#key-concepts","title":"Key Concepts","text":""},{"location":"probabilistic-tracking/#hidden-markov-model-hmm","title":"Hidden Markov Model (HMM)","text":"<p>Components: 1. Hidden States (X): Ghost positions (unobservable) 2. Observations (E): Noisy distance readings (observable) 3. Transition Model: P(X_t | X_{t-1}) - how ghosts move 4. Sensor Model: P(E_t | X_t) - observation noise distribution 5. Initial Distribution: P(X_0) - prior belief</p> <p>Inference Goal: <pre><code>Compute: P(X_t | e_{1:t})  (posterior belief)\nGiven: e_1, e_2, ..., e_t   (observation sequence)\n</code></pre></p>"},{"location":"probabilistic-tracking/#bayes-rule","title":"Bayes Rule","text":"<p>Update belief with new observation: <pre><code>P(X | e) = \u03b1 P(e | X) P(X)\n\nwhere:\n  P(X) = prior belief\n  P(e | X) = observation likelihood\n  P(X | e) = posterior belief\n  \u03b1 = normalization constant\n</code></pre></p>"},{"location":"probabilistic-tracking/#forward-algorithm","title":"Forward Algorithm","text":"<p>Recursive belief propagation: <pre><code>P(X_t | e_{1:t}) = \u03b1 P(e_t | X_t) \u03a3_{x_{t-1}} P(X_t | x_{t-1}) P(x_{t-1} | e_{1:t-1})\n                   \u2514\u2500 Observe \u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Time Elapse \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Two-Step Process: 1. Predict: Use transition model to forecast next position 2. Update: Use observation to correct prediction</p>"},{"location":"probabilistic-tracking/#algorithm-implementations","title":"Algorithm Implementations","text":""},{"location":"probabilistic-tracking/#exact-inference_1","title":"Exact Inference","text":"<p>Initialize Beliefs: <pre><code>def initializeUniformly(self, gameState):\n    \"\"\"Start with uniform prior\"\"\"\n    self.beliefs = util.Counter()\n    for pos in self.legalPositions:\n        self.beliefs[pos] = 1.0\n    self.beliefs.normalize()\n</code></pre></p> <p>Observation Update: <pre><code>def observeUpdate(self, observation, gameState):\n    \"\"\"Apply Bayes rule\"\"\"\n    pacmanPos = gameState.getPacmanPosition()\n\n    for pos in self.legalPositions:\n        # Observation likelihood P(e_t | X_t)\n        observationProb = self.getObservationProb(\n            observation, pacmanPos, pos, self.index\n        )\n\n        # Bayesian update\n        self.beliefs[pos] *= observationProb\n\n    self.beliefs.normalize()\n</code></pre></p> <p>Time Elapse: <pre><code>def elapseTime(self, gameState):\n    \"\"\"Predict next belief state\"\"\"\n    newBeliefs = util.Counter()\n\n    for oldPos in self.legalPositions:\n        # Transition distribution P(X_t | x_{t-1})\n        newPosDist = self.getPositionDistribution(gameState, oldPos)\n\n        for newPos, prob in newPosDist.items():\n            # Sum over old positions\n            newBeliefs[newPos] += prob * self.beliefs[oldPos]\n\n    self.beliefs = newBeliefs\n    self.beliefs.normalize()\n</code></pre></p>"},{"location":"probabilistic-tracking/#particle-filtering_2","title":"Particle Filtering","text":"<p>Initialize Particles: <pre><code>def initializeUniformly(self, gameState):\n    \"\"\"Sample N particles uniformly\"\"\"\n    self.particles = []\n    for i in range(self.numParticles):\n        self.particles.append(random.choice(self.legalPositions))\n</code></pre></p> <p>Time Elapse (Prediction): <pre><code>def elapseTime(self, gameState):\n    \"\"\"Move particles via transition model\"\"\"\n    newParticles = []\n\n    for particle in self.particles:\n        # Sample new position from transition\n        newPosDist = self.getPositionDistribution(gameState, particle)\n        newParticle = util.sample(newPosDist)\n        newParticles.append(newParticle)\n\n    self.particles = newParticles\n</code></pre></p> <p>Observation Update (Correction): <pre><code>def observeUpdate(self, observation, gameState):\n    \"\"\"Weight and resample particles\"\"\"\n    pacmanPos = gameState.getPacmanPosition()\n    weights = []\n\n    # Calculate importance weights\n    for particle in self.particles:\n        weight = self.getObservationProb(\n            observation, pacmanPos, particle, self.index\n        )\n        weights.append(weight)\n\n    # Handle particle depletion\n    if sum(weights) == 0:\n        self.initializeUniformly(gameState)\n        return\n\n    # Resample with replacement\n    self.particles = util.nSample(weights, self.particles, self.numParticles)\n</code></pre></p> <p>Extract Beliefs: <pre><code>def getBeliefDistribution(self):\n    \"\"\"Convert particles to probability distribution\"\"\"\n    beliefs = util.Counter()\n    for particle in self.particles:\n        beliefs[particle] += 1.0\n    beliefs.normalize()\n    return beliefs\n</code></pre></p>"},{"location":"probabilistic-tracking/#joint-particle-filtering_2","title":"Joint Particle Filtering","text":"<p>Joint State: <pre><code># Each particle represents all ghost positions\nparticle = (ghost0_pos, ghost1_pos, ghost2_pos, ...)\n</code></pre></p> <p>Joint Observation Update: <pre><code>def observeUpdate(self, observation, gameState):\n    \"\"\"Update joint distribution\"\"\"\n    weights = []\n\n    for particle in self.particles:\n        weight = 1.0\n\n        # Multiply likelihoods for all ghosts\n        for i in range(self.numGhosts):\n            ghostPos = particle[i]\n            obs = observation[i]\n            weight *= self.getObservationProb(obs, pacmanPos, ghostPos, i)\n\n        weights.append(weight)\n\n    # Resample joint particles\n    self.particles = util.nSample(weights, self.particles, self.numParticles)\n</code></pre></p> <p>Independent Time Elapse: <pre><code>def elapseTime(self, gameState):\n    \"\"\"Move all ghosts independently\"\"\"\n    newParticles = []\n\n    for oldParticle in self.particles:\n        newParticle = []\n\n        # Sample new position for each ghost\n        for i in range(self.numGhosts):\n            oldPos = oldParticle[i]\n            newPosDist = self.getPositionDistribution(gameState, oldPos, i)\n            newPos = util.sample(newPosDist)\n            newParticle.append(newPos)\n\n        newParticles.append(tuple(newParticle))\n\n    self.particles = newParticles\n</code></pre></p>"},{"location":"probabilistic-tracking/#performance-considerations","title":"Performance Considerations","text":""},{"location":"probabilistic-tracking/#exact-inference_2","title":"Exact Inference","text":"<p>Advantages: - \u2705 Exact probability distribution - \u2705 No sampling approximation - \u2705 Guaranteed correct beliefs</p> <p>Limitations: - \u26a0\ufe0f O(|X|\u00b2) per update limits scalability - \u26a0\ufe0f Memory grows linearly with state space - \u26a0\ufe0f Impractical for &gt;1000-2000 positions</p> <p>When to Use: Small state spaces where exactness is critical</p>"},{"location":"probabilistic-tracking/#particle-filtering_3","title":"Particle Filtering","text":"<p>Advantages: - \u2705 Scales to large state spaces - \u2705 O(N) complexity per update - \u2705 Real-time capable (30+ FPS) - \u2705 Memory efficient</p> <p>Limitations: - \u26a0\ufe0f Approximate (sampling error) - \u26a0\ufe0f Can suffer particle degeneracy - \u26a0\ufe0f Requires tuning particle count - \u26a0\ufe0f May need reinitialization</p> <p>When to Use: Large state spaces, real-time requirements, approximate beliefs acceptable</p>"},{"location":"probabilistic-tracking/#joint-particle-filtering_3","title":"Joint Particle Filtering","text":"<p>Advantages: - \u2705 Handles multiple targets - \u2705 Captures target correlations - \u2705 Scalable with particle count</p> <p>Limitations: - \u26a0\ufe0f Exponentially harder with more targets - \u26a0\ufe0f Requires many more particles (500-2000) - \u26a0\ufe0f Slower updates than single-target</p> <p>When to Use: Multiple targets where joint distribution matters</p>"},{"location":"probabilistic-tracking/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"probabilistic-tracking/#automated-testing","title":"Automated Testing","text":"<pre><code># All tests\npython autograder.py\n\n# Individual components\npython autograder.py -q q1  # Bayes Net Construction\npython autograder.py -q q2  # Exact Observe Update\npython autograder.py -q q3  # Exact Time Elapse\npython autograder.py -q q4  # Particle Filtering\npython autograder.py -q q5  # Joint Particle Filter\n\n# Verbose output\npython autograder.py -q q4 --verbose\n</code></pre>"},{"location":"probabilistic-tracking/#manual-evaluation","title":"Manual Evaluation","text":"<pre><code># Visual debugging\npython busters.py -p ParticleAgent -l smallClassic -k 1 --frameTime=0.2\n\n# Performance testing\npython busters.py -p JointParticleAgent -k 2 -n 20 -q\n\n# Compare methods\npython busters.py -p BasicAgentAA -l trickyClassic -k 1 -n 10 -q\npython busters.py -p ParticleAgent -l trickyClassic -k 1 -n 10 -q\n</code></pre>"},{"location":"probabilistic-tracking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"probabilistic-tracking/#beliefs-stay-uniform","title":"Beliefs Stay Uniform","text":"<p>Problem: No belief update after observations</p> <p>Solutions: - Check observation probability calculation - Verify beliefs multiply (not replace) observation likelihood - Ensure normalization is called after update</p>"},{"location":"probabilistic-tracking/#particle-degeneracy","title":"Particle Degeneracy","text":"<p>Problem: All particles collapse to single location</p> <p>Solutions: - Increase particle count (try 500-1000) - Check transition model provides sufficient spread - Add particle diversity in resampling - Implement reinitialization on all-zero weights</p>"},{"location":"probabilistic-tracking/#joint-tracking-fails","title":"Joint Tracking Fails","text":"<p>Problem: Poor performance with multiple ghosts</p> <p>Solutions: - Significantly increase particles (1000-2000) - Verify joint observation probabilities multiply correctly - Check each ghost moves independently in time elapse - Test single-ghost tracking first</p>"},{"location":"probabilistic-tracking/#slow-performance","title":"Slow Performance","text":"<p>Problem: Low frame rate, laggy updates</p> <p>Solutions: - Reduce particle count - Optimize observation probability calculation - Use smaller maze layouts - Profile code for bottlenecks</p>"},{"location":"probabilistic-tracking/#learning-objectives","title":"Learning Objectives","text":"<p>\u2705 Bayesian Networks - Construct graphical models for uncertainty - Understand conditional independence - Define conditional probability tables (CPTs)</p> <p>\u2705 Inference Algorithms - Implement exact inference with forward algorithm - Understand belief propagation in temporal models - Apply Bayes rule for observation updates</p> <p>\u2705 Particle Filtering - Understand importance sampling - Implement resampling algorithms - Handle particle degeneracy - Balance accuracy vs computational cost</p> <p>\u2705 Multi-Target Tracking - Represent joint distributions - Update beliefs for multiple targets - Handle conditional independence assumptions</p>"},{"location":"probabilistic-tracking/#additional-resources","title":"Additional Resources","text":""},{"location":"probabilistic-tracking/#useful-classes-and-methods","title":"Useful Classes and Methods","text":"<p>Counter Operations: <pre><code>counter = util.Counter()          # Dict with default 0\ncounter.normalize()               # Convert to probabilities\ncounter.argMax()                  # Key with maximum value\ncounter.totalCount()              # Sum of all values\n</code></pre></p> <p>Sampling Functions: <pre><code>util.sample(distribution)         # Sample once from distribution\nutil.nSample(weights, values, n)  # Sample n times with weights\nutil.sampleFromCounter(counter)   # Sample from Counter object\n</code></pre></p> <p>Game State Methods: <pre><code>gameState.getPacmanPosition()     # Pacman's (x, y) position\ngameState.getGhostPositions()     # List of ghost positions\ngameState.getLivingGhosts()       # Which ghosts are still active\n</code></pre></p>"},{"location":"probabilistic-tracking/#references","title":"References","text":"<ul> <li>UC Berkeley CS188 - Original project framework</li> <li>Russell &amp; Norvig - \"Artificial Intelligence: A Modern Approach\" (Chapter 15)</li> <li>Daphne Koller - \"Probabilistic Graphical Models\"</li> <li>Sebastian Thrun - \"Probabilistic Robotics\"</li> </ul>"},{"location":"reinforcement-learning/","title":"Reinforcement Learning","text":""},{"location":"reinforcement-learning/#overview","title":"Overview","text":"<p>The Reinforcement Learning project implements value-based learning methods where agents discover optimal policies through trial-and-error interaction with environments. Built on the foundations of Markov Decision Processes (MDPs), the system explores both model-based (Value Iteration) and model-free (Q-Learning) algorithms, culminating in function approximation techniques for handling complex state spaces.</p> <p>The implementation covers three distinct environments (Gridworld, Crawler Robot, Pacman) and demonstrates how agents can learn from experience without explicit instruction, achieving optimal policies through systematic exploration and temporal difference updates.</p>"},{"location":"reinforcement-learning/#system-architecture","title":"System Architecture","text":"<p>The reinforcement learning system follows a learning pipeline from basic value iteration to advanced function approximation:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  REINFORCEMENT LEARNING SYSTEM                              \u2502\n\u2502                       System Workflow Diagram                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502      ENVIRONMENT                 \u2502\n                    \u2502  - Gridworld                     \u2502\n                    \u2502  - Crawler Robot                 \u2502\n                    \u2502  - Pacman Game                   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   STATE &amp; ACTIONS            \u2502\n                    \u2502  - Current State (s)         \u2502\n                    \u2502  - Available Actions (a)     \u2502\n                    \u2502  - Transition Model T(s,a,s')\u2502\n                    \u2502  - Reward Function R(s,a,s') \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                          \u2502                            \u2502\n        \u25bc                          \u25bc                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 VALUE          \u2502      \u2502 Q-LEARNING     \u2502        \u2502 APPROXIMATE    \u2502\n\u2502 ITERATION      \u2502      \u2502                \u2502        \u2502 Q-LEARNING     \u2502\n\u2502                \u2502      \u2502 - Model-free   \u2502        \u2502                \u2502\n\u2502 - Offline      \u2502      \u2502 - Temporal     \u2502        \u2502 - Function     \u2502\n\u2502   planning     \u2502      \u2502   difference   \u2502        \u2502   approximation\u2502\n\u2502 - Full model   \u2502      \u2502 - Experience   \u2502        \u2502 - Feature-based\u2502\n\u2502 - Bellman eqn  \u2502      \u2502   replay       \u2502        \u2502 - Generalization\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                         \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   POLICY EXTRACTION     \u2502\n                    \u2502   \u03c0(s) = argmax_a Q(s,a)\u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   LEARNED POLICY        \u2502\n                    \u2502   - Optimal actions     \u2502\n                    \u2502   - Value estimates     \u2502\n                    \u2502   - State preferences   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nKey Algorithms:\n\u251c\u2500 Value Iteration: V(s) = max_a \u03a3 T(s,a,s')[R(s,a,s') + \u03b3V(s')]\n\u251c\u2500 Q-Learning: Q(s,a) \u2190 Q(s,a) + \u03b1[R + \u03b3 max_a' Q(s',a') - Q(s,a)]\n\u2514\u2500 Approximate Q-Learning: Q(s,a) = \u03a3 w_i \u00d7 f_i(s,a)\n</code></pre>"},{"location":"reinforcement-learning/#features-and-capabilities","title":"Features and Capabilities","text":""},{"location":"reinforcement-learning/#learning-algorithms","title":"Learning Algorithms","text":""},{"location":"reinforcement-learning/#1-value-iteration","title":"1. Value Iteration","text":"<ul> <li>Type: Model-based offline planning</li> <li>Method: Dynamic programming with Bellman updates</li> <li>Requires: Complete knowledge of T(s,a,s') and R(s,a,s')</li> <li>Convergence: Guaranteed with sufficient iterations</li> <li>Complexity: O(|S|\u00b2 \u00d7 |A|) per iteration</li> <li>Best For: Known MDP, offline planning acceptable</li> </ul>"},{"location":"reinforcement-learning/#2-q-learning","title":"2. Q-Learning","text":"<ul> <li>Type: Model-free online learning</li> <li>Method: Temporal difference updates on Q-values</li> <li>Requires: Only ability to interact with environment</li> <li>Convergence: Guaranteed with proper learning schedule</li> <li>Complexity: O(1) per experience tuple</li> <li>Best For: Unknown environment, online learning needed</li> </ul>"},{"location":"reinforcement-learning/#3-approximate-q-learning","title":"3. Approximate Q-Learning","text":"<ul> <li>Type: Model-free with function approximation</li> <li>Method: Linear combination of feature weights</li> <li>Requires: Feature extractor design</li> <li>Convergence: Approximate (function approximation error)</li> <li>Complexity: O(k) for k features</li> <li>Best For: Large state spaces, generalization needed</li> </ul>"},{"location":"reinforcement-learning/#learning-environments","title":"Learning Environments","text":""},{"location":"reinforcement-learning/#gridworld","title":"Gridworld","text":"<ul> <li>Description: Simple grid navigation</li> <li>States: Grid cells (x, y)</li> <li>Actions: North, South, East, West</li> <li>Rewards: Goal (+10), cliff (-10), living cost (-0.1)</li> <li>Purpose: Algorithm visualization and debugging</li> </ul>"},{"location":"reinforcement-learning/#crawler-robot","title":"Crawler Robot","text":"<ul> <li>Description: 2-link arm locomotion</li> <li>States: Joint angles (arm, hand)</li> <li>Actions: Increment/decrement angles</li> <li>Rewards: Forward movement distance</li> <li>Purpose: Continuous control and exploration</li> </ul>"},{"location":"reinforcement-learning/#pacman","title":"Pacman","text":"<ul> <li>Description: Classic game environment</li> <li>States: Position, ghosts, food grid</li> <li>Actions: North, South, East, West</li> <li>Rewards: Food (+10), ghost penalty (-500), win (+500)</li> <li>Purpose: Complex decision-making</li> </ul>"},{"location":"reinforcement-learning/#quick-start","title":"Quick Start","text":""},{"location":"reinforcement-learning/#installation","title":"Installation","text":"<p>Navigate to project directory: <pre><code>cd 2_reinforcement\n</code></pre></p>"},{"location":"reinforcement-learning/#running-learning-agents","title":"Running Learning Agents","text":""},{"location":"reinforcement-learning/#value-iteration-on-gridworld","title":"Value Iteration on Gridworld","text":"<pre><code># Run 100 iterations\npython gridworld.py -a value -i 100 -k 10\n\n# With custom discount factor\npython gridworld.py -a value -i 100 -g 0.95\n</code></pre>"},{"location":"reinforcement-learning/#q-learning-on-gridworld","title":"Q-Learning on Gridworld","text":"<pre><code># Train for 100 episodes\npython gridworld.py -a q -k 100\n\n# Custom learning parameters\npython gridworld.py -a q -k 100 -e 0.1 -l 0.5 -g 0.9\n</code></pre>"},{"location":"reinforcement-learning/#crawler-robot-learning","title":"Crawler Robot Learning","text":"<pre><code># Watch robot learn to crawl\npython crawler.py\n</code></pre>"},{"location":"reinforcement-learning/#q-learning-on-pacman","title":"Q-Learning on Pacman","text":"<pre><code># Train for 2000 episodes, test for 10\npython pacman.py -p PacmanQAgent -x 2000 -n 2010 -l smallGrid\n</code></pre>"},{"location":"reinforcement-learning/#approximate-q-learning-on-pacman","title":"Approximate Q-Learning on Pacman","text":"<pre><code># Train with feature approximation\npython pacman.py -p ApproximateQAgent -a extractor=SimpleExtractor -x 50 -n 60 -l mediumGrid\n</code></pre>"},{"location":"reinforcement-learning/#command-line-options","title":"Command Line Options","text":"Option Description Values <code>-a</code> Agent type <code>value</code>, <code>q</code>, <code>random</code> <code>-k</code> Number of episodes <code>10</code>, <code>100</code>, <code>1000</code> <code>-i</code> Iterations (value iteration) <code>10</code>, <code>50</code>, <code>100</code> <code>-g</code> Discount factor (\u03b3) <code>0.1</code> to <code>0.99</code> <code>-l</code> Learning rate (\u03b1) <code>0.1</code> to <code>1.0</code> <code>-e</code> Exploration rate (\u03b5) <code>0.0</code> to <code>1.0</code> <code>-x</code> Training episodes <code>50</code>, <code>2000</code> <code>-n</code> Total episodes Must be &gt; <code>-x</code>"},{"location":"reinforcement-learning/#algorithm-details","title":"Algorithm Details","text":""},{"location":"reinforcement-learning/#value-iteration","title":"Value Iteration","text":"<p>Bellman Optimality Equation: <pre><code>V*(s) = max_a \u03a3_{s'} T(s,a,s') [R(s,a,s') + \u03b3 V*(s')]\n</code></pre></p> <p>Algorithm: <pre><code>1. Initialize V_0(s) = 0 for all states\n2. For iteration k = 1 to K:\n       For each state s:\n           Q_k(s,a) = \u03a3_{s'} T(s,a,s')[R(s,a,s') + \u03b3 V_{k-1}(s')]\n           V_k(s) = max_a Q_k(s,a)\n3. Extract policy: \u03c0(s) = argmax_a Q(s,a)\n</code></pre></p> <p>Implementation: <pre><code>def runValueIteration(self):\n    for iteration in range(self.iterations):\n        newValues = util.Counter()\n\n        for state in self.mdp.getStates():\n            if self.mdp.isTerminal(state):\n                newValues[state] = 0\n                continue\n\n            actions = self.mdp.getPossibleActions(state)\n            if actions:\n                qValues = [self.computeQValueFromValues(state, a) for a in actions]\n                newValues[state] = max(qValues)\n\n        self.values = newValues\n</code></pre></p> <p>Properties: - Offline algorithm (no interaction needed) - Synchronous updates - Converges to optimal values - Requires full MDP model</p>"},{"location":"reinforcement-learning/#q-learning","title":"Q-Learning","text":"<p>Update Rule: <pre><code>Q(s,a) \u2190 Q(s,a) + \u03b1 [R + \u03b3 max_{a'} Q(s',a') - Q(s,a)]\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500 TD target \u2500\u2500\u2500\u2500\u2518 \u2514\u2500 Old Q \u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 TD error \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Components: - \u03b1 (alpha): Learning rate (0 &lt; \u03b1 \u2264 1) - \u03b3 (gamma): Discount factor (0 \u2264 \u03b3 &lt; 1) - \u03b5 (epsilon): Exploration rate for \u03b5-greedy</p> <p>Algorithm: <pre><code>1. Initialize Q(s,a) = 0 for all s,a\n2. For each episode:\n       s = initial state\n       While s not terminal:\n           # \u03b5-greedy action selection\n           if random() &lt; \u03b5:\n               a = random action\n           else:\n               a = argmax_{a'} Q(s,a')\n\n           # Take action, observe outcome\n           s', r = environment.step(a)\n\n           # TD update\n           target = r + \u03b3 \u00d7 max_{a'} Q(s',a')\n           Q(s,a) \u2190 Q(s,a) + \u03b1 \u00d7 (target - Q(s,a))\n\n           s = s'\n</code></pre></p> <p>Implementation: <pre><code>def update(self, state, action, nextState, reward):\n    \"\"\"Temporal difference update\"\"\"\n    # Current Q-value\n    currentQ = self.getQValue(state, action)\n\n    # TD target\n    maxNextQ = self.computeValueFromQValues(nextState)\n    sample = reward + self.discount * maxNextQ\n\n    # TD update\n    self.qValues[(state, action)] = currentQ + self.alpha * (sample - currentQ)\n</code></pre></p> <p>Properties: - Model-free (learns from experience) - Online learning - Off-policy (learns optimal while exploring) - Converges with decreasing \u03b1 and sufficient exploration</p>"},{"location":"reinforcement-learning/#approximate-q-learning","title":"Approximate Q-Learning","text":"<p>Linear Function Approximation: <pre><code>Q(s,a) = w_1 \u00d7 f_1(s,a) + w_2 \u00d7 f_2(s,a) + ... + w_n \u00d7 f_n(s,a)\n       = \u03a3_i w_i \u00d7 f_i(s,a)\n</code></pre></p> <p>Weight Update: <pre><code>difference = [R + \u03b3 max_{a'} Q(s',a')] - Q(s,a)\nw_i \u2190 w_i + \u03b1 \u00d7 difference \u00d7 f_i(s,a)\n</code></pre></p> <p>Algorithm: <pre><code>def update(self, state, action, nextState, reward):\n    \"\"\"Update feature weights\"\"\"\n    # Extract features\n    features = self.featExtractor.getFeatures(state, action)\n\n    # Current Q-value (from weights and features)\n    currentQ = sum(self.weights[f] * features[f] for f in features)\n\n    # TD target\n    maxNextQ = self.computeValueFromQValues(nextState)\n    target = reward + self.discount * maxNextQ\n\n    # TD error\n    difference = target - currentQ\n\n    # Update weights\n    for feature in features:\n        self.weights[feature] += self.alpha * difference * features[feature]\n</code></pre></p> <p>Feature Design Example (Pacman): <pre><code>def getFeatures(self, state, action):\n    features = util.Counter()\n\n    # Bias term\n    features[\"bias\"] = 1.0\n\n    # Distance to nearest food\n    successor = state.generateSuccessor(0, action)\n    foodList = successor.getFood().asList()\n    if foodList:\n        minDist = min([manhattanDistance(successor.getPacmanPosition(), f)\n                       for f in foodList])\n        # Normalize by maze size\n        features[\"closest-food\"] = float(minDist) / (walls.width * walls.height)\n\n    # Ghost proximity\n    ghosts = [successor.getGhostPosition(i) for i in range(1, successor.getNumAgents())]\n    features[\"#-of-ghosts-1-step-away\"] = sum(\n        manhattanDistance(successor.getPacmanPosition(), g) &lt;= 1 for g in ghosts\n    )\n\n    # Food consumption\n    features[\"eats-food\"] = 1.0 if successor.getFood()[x][y] else 0.0\n\n    return features\n</code></pre></p> <p>Properties: - Generalizes to unseen states - Learns from fewer samples - Scalable to large state spaces - Feature quality crucial</p>"},{"location":"reinforcement-learning/#performance-analysis","title":"Performance Analysis","text":""},{"location":"reinforcement-learning/#value-iteration_1","title":"Value Iteration","text":"<p>Convergence:</p> Iterations Max Value Change Policy Quality 10 2.5 Poor 50 0.1 Good 100 0.001 Optimal <p>Computational Cost: - Time per iteration: O(|S|\u00b2 \u00d7 |A|) - Typical iterations: 10-100 - Memory: O(|S|) for value table</p>"},{"location":"reinforcement-learning/#q-learning_1","title":"Q-Learning","text":"<p>Training Progress:</p> Episodes Win Rate (Pacman) Avg Score 100 20% -100 500 50% +50 1000 70% +200 2000 85% +400 <p>Hyperparameter Impact:</p> Parameter Value Effect \u03b1 (learning) 0.1 Slow, stable \u03b1 (learning) 0.5 Fast, oscillations \u03b3 (discount) 0.8 Short-term focus \u03b3 (discount) 0.99 Long-term planning \u03b5 (exploration) 0.1 Mostly exploit \u03b5 (exploration) 0.5 Heavy exploration"},{"location":"reinforcement-learning/#approximate-q-learning_1","title":"Approximate Q-Learning","text":"<p>Sample Efficiency:</p> Method Episodes to 70% Win Memory Tabular Q-Learning 2000 O(|S| \u00d7 |A|) Approximate Q-Learning 200 O(k features) <p>Speedup: 10x faster convergence with good features</p>"},{"location":"reinforcement-learning/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"reinforcement-learning/#value-iteration_2","title":"Value Iteration","text":"<pre><code># Run on gridworld with visualization\npython gridworld.py -a value -i 100 -k 10\n\n# Adjust discount factor\npython gridworld.py -a value -i 100 -g 0.9\n\n# Manual control to test learned policy\npython gridworld.py -a value -i 100 -m\n</code></pre> <p>Controls: - Arrow keys: Manual navigation - <code>a</code>: Auto-run learned policy - <code>q</code>: Quit</p>"},{"location":"reinforcement-learning/#q-learning_2","title":"Q-Learning","text":"<pre><code># Train on gridworld\npython gridworld.py -a q -k 100\n\n# Custom hyperparameters\npython gridworld.py -a q -k 200 -l 0.3 -e 0.1 -g 0.95\n\n# Bridge crossing challenge\npython gridworld.py -a q -k 50 -n 0.01 -g 1 -l 1 -e 0.1 -b BridgeGrid\n</code></pre>"},{"location":"reinforcement-learning/#crawler-robot_1","title":"Crawler Robot","text":"<pre><code># Watch robot learn locomotion\npython crawler.py\n\n# Use GUI to adjust learning parameters\n# - Learning rate slider\n# - Discount factor slider\n# - Exploration rate slider\n# - Step delay for visualization\n</code></pre>"},{"location":"reinforcement-learning/#pacman-q-learning","title":"Pacman Q-Learning","text":"<pre><code># Train tabular Q-learning\npython pacman.py -p PacmanQAgent -x 2000 -n 2010 -l smallGrid\n\n# Fast training (no graphics)\npython pacman.py -p PacmanQAgent -x 2000 -n 2010 -q\n</code></pre>"},{"location":"reinforcement-learning/#approximate-q-learning_2","title":"Approximate Q-Learning","text":"<pre><code># Train with simple features\npython pacman.py -p ApproximateQAgent -a extractor=SimpleExtractor -x 50 -n 60 -l mediumGrid\n\n# Different layouts\npython pacman.py -p ApproximateQAgent -x 100 -n 110 -l mediumClassic\n</code></pre>"},{"location":"reinforcement-learning/#key-concepts-explained","title":"Key Concepts Explained","text":""},{"location":"reinforcement-learning/#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>Components: - S: Set of states - A: Set of actions - T(s,a,s'): Transition function P(s'|s,a) - R(s,a,s'): Reward function - \u03b3: Discount factor (0 \u2264 \u03b3 &lt; 1)</p> <p>Objective: <pre><code>Find policy \u03c0*: S \u2192 A that maximizes:\nV^\u03c0(s) = E[\u03a3_{t=0}^\u221e \u03b3^t R_t | \u03c0]\n</code></pre></p>"},{"location":"reinforcement-learning/#bellman-equations","title":"Bellman Equations","text":"<p>Bellman Optimality for V*: <pre><code>V*(s) = max_a Q*(s,a)\nQ*(s,a) = \u03a3_{s'} T(s,a,s')[R(s,a,s') + \u03b3 V*(s')]\n</code></pre></p> <p>Bellman Optimality for Q*: <pre><code>Q*(s,a) = \u03a3_{s'} T(s,a,s')[R(s,a,s') + \u03b3 max_{a'} Q*(s',a')]\n</code></pre></p>"},{"location":"reinforcement-learning/#temporal-difference-learning","title":"Temporal Difference Learning","text":"<p>TD Error: <pre><code>\u03b4_t = R_t + \u03b3 V(S_{t+1}) - V(S_t)\n     \u2514\u2500\u2500\u2500 TD target \u2500\u2500\u2518   \u2514\u2500 Current \u2500\u2518\n</code></pre></p> <p>Update: <pre><code>V(S_t) \u2190 V(S_t) + \u03b1 \u00d7 \u03b4_t\n</code></pre></p> <p>Properties: - Learns from incomplete episodes - Bootstraps from current estimates - Balances bias (bootstrapping) vs variance (Monte Carlo)</p>"},{"location":"reinforcement-learning/#exploration-vs-exploitation","title":"Exploration vs Exploitation","text":"<p>\u03b5-greedy Policy: <pre><code>a = { random action           with probability \u03b5\n    { argmax_a Q(s,a)         with probability 1-\u03b5\n</code></pre></p> <p>Exploration Strategies: - Fixed \u03b5: Constant exploration (e.g., \u03b5=0.1) - Decaying \u03b5: Start high (\u03b5=1.0), decay to low (\u03b5=0.01) - Optimistic initialization: Q(s,a) = high value encourages exploration</p>"},{"location":"reinforcement-learning/#implementation-examples","title":"Implementation Examples","text":""},{"location":"reinforcement-learning/#value-iteration_3","title":"Value Iteration","text":"<pre><code>class ValueIterationAgent:\n    def __init__(self, mdp, discount=0.9, iterations=100):\n        self.mdp = mdp\n        self.discount = discount\n        self.iterations = iterations\n        self.values = util.Counter()\n        self.runValueIteration()\n\n    def runValueIteration(self):\n        \"\"\"Perform Bellman updates\"\"\"\n        for iteration in range(self.iterations):\n            newValues = util.Counter()\n\n            for state in self.mdp.getStates():\n                if self.mdp.isTerminal(state):\n                    continue\n\n                actions = self.mdp.getPossibleActions(state)\n                if actions:\n                    qValues = [self.computeQValue(state, a) for a in actions]\n                    newValues[state] = max(qValues)\n\n            self.values = newValues\n\n    def computeQValue(self, state, action):\n        \"\"\"Compute Q(s,a) from V(s')\"\"\"\n        qValue = 0\n        for nextState, prob in self.mdp.getTransitionStatesAndProbs(state, action):\n            reward = self.mdp.getReward(state, action, nextState)\n            qValue += prob * (reward + self.discount * self.values[nextState])\n        return qValue\n</code></pre>"},{"location":"reinforcement-learning/#q-learning_3","title":"Q-Learning","text":"<pre><code>class QLearningAgent:\n    def __init__(self, alpha=0.5, epsilon=0.1, gamma=0.9):\n        self.alpha = alpha      # Learning rate\n        self.epsilon = epsilon  # Exploration rate\n        self.discount = gamma   # Discount factor\n        self.qValues = util.Counter()\n\n    def getQValue(self, state, action):\n        \"\"\"Return Q(s,a)\"\"\"\n        return self.qValues[(state, action)]\n\n    def getAction(self, state):\n        \"\"\"\u03b5-greedy action selection\"\"\"\n        legalActions = self.getLegalActions(state)\n\n        # Exploration\n        if util.flipCoin(self.epsilon):\n            return random.choice(legalActions)\n\n        # Exploitation\n        return self.computeActionFromQValues(state)\n\n    def update(self, state, action, nextState, reward):\n        \"\"\"TD update\"\"\"\n        currentQ = self.getQValue(state, action)\n        maxNextQ = max([self.getQValue(nextState, a)\n                       for a in self.getLegalActions(nextState)] or [0])\n\n        sample = reward + self.discount * maxNextQ\n        self.qValues[(state, action)] = currentQ + self.alpha * (sample - currentQ)\n</code></pre>"},{"location":"reinforcement-learning/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"reinforcement-learning/#automated-testing","title":"Automated Testing","text":"<pre><code># All questions\npython autograder.py\n\n# Individual questions\npython autograder.py -q q1  # Value Iteration\npython autograder.py -q q2  # Bridge Crossing\npython autograder.py -q q3  # Q-Learning Discount\npython autograder.py -q q4  # Q-Learning Pacman\npython autograder.py -q q5  # Approximate Q-Learning\npython autograder.py -q q6  # Analysis Questions\n\n# Verbose output for debugging\npython autograder.py -q q1 --verbose\n</code></pre>"},{"location":"reinforcement-learning/#manual-testing","title":"Manual Testing","text":"<pre><code># Watch value iteration converge\npython gridworld.py -a value -i 10 -s 0.5  # 0.5s pause per iteration\n\n# Train Q-learning and visualize\npython gridworld.py -a q -k 50 -s 0.2\n\n# Test different learning rates\nfor lr in 0.1 0.3 0.5 0.8; do\n    echo \"Testing learning rate $lr\"\n    python gridworld.py -a q -k 100 -l $lr -q\ndone\n</code></pre>"},{"location":"reinforcement-learning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reinforcement-learning/#q-learning-not-converging","title":"Q-Learning Not Converging","text":"<p>Problem: Q-values oscillate or don't stabilize</p> <p>Solutions: - Increase training episodes (try 1000-5000) - Decrease learning rate (\u03b1 = 0.1-0.3) - Ensure sufficient exploration (\u03b5 = 0.05-0.2) - Check for negative reward cycles - Verify discount factor &lt; 1.0</p>"},{"location":"reinforcement-learning/#pacman-performs-poorly","title":"Pacman Performs Poorly","text":"<p>Problem: Low win rate after training</p> <p>Solutions: - Train for more episodes (2000-5000) - Adjust reward shaping - Check feature extraction (approximate QL) - Increase exploration during training - Test on simpler layouts first</p>"},{"location":"reinforcement-learning/#approximate-q-learning-doesnt-generalize","title":"Approximate Q-Learning Doesn't Generalize","text":"<p>Problem: Poor performance on new states</p> <p>Solutions: - Design better features (add ghost distance, food count) - Normalize feature values (divide by maze size) - Increase feature diversity - Train longer (100+ episodes) - Check feature weights make sense</p>"},{"location":"reinforcement-learning/#value-iteration-doesnt-converge","title":"Value Iteration Doesn't Converge","text":"<p>Problem: Values keep changing significantly</p> <p>Solutions: - Increase iterations (100-200) - Check discount factor &lt; 1.0 - Verify synchronous updates (copy values) - Ensure terminal states handled correctly</p>"},{"location":"reinforcement-learning/#learning-objectives","title":"Learning Objectives","text":"<p>\u2705 MDP Fundamentals - Understand states, actions, transitions, rewards - Formulate problems as MDPs - Compute optimal policies from value functions</p> <p>\u2705 Value Iteration - Implement Bellman optimality updates - Extract policies from value functions - Analyze convergence properties</p> <p>\u2705 Q-Learning - Implement model-free TD learning - Design exploration strategies (\u03b5-greedy) - Understand off-policy learning - Handle experience replay</p> <p>\u2705 Function Approximation - Design feature extractors - Implement linear function approximation - Understand generalization vs memorization - Balance feature complexity and learnability</p>"},{"location":"reinforcement-learning/#additional-resources","title":"Additional Resources","text":""},{"location":"reinforcement-learning/#mdp-interface","title":"MDP Interface","text":"<pre><code>mdp.getStates()                          # All states in MDP\nmdp.getStartState()                      # Initial state\nmdp.getPossibleActions(state)            # Available actions from state\nmdp.getTransitionStatesAndProbs(s, a)    # Returns [(s', prob), ...]\nmdp.getReward(state, action, nextState)  # R(s,a,s')\nmdp.isTerminal(state)                    # Check if terminal\n</code></pre>"},{"location":"reinforcement-learning/#reinforcementagent-methods","title":"ReinforcementAgent Methods","text":"<pre><code>self.getLegalActions(state)    # Valid actions\nself.getValue(state)           # V(s) from Q-values\nself.getQValue(state, action)  # Q(s,a)\nself.getPolicy(state)          # \u03c0(s) = best action\n</code></pre>"},{"location":"reinforcement-learning/#references","title":"References","text":"<ul> <li>UC Berkeley CS188 - Original project framework</li> <li>Sutton &amp; Barto - \"Reinforcement Learning: An Introduction\" (2<sup>nd</sup> Edition)</li> <li>Russell &amp; Norvig - \"Artificial Intelligence: A Modern Approach\" (Chapter 17, 22)</li> <li>Course Materials - CS-5100 lecture slides on MDPs and RL</li> </ul>"},{"location":"search-algorithms/","title":"Search Algorithms","text":""},{"location":"search-algorithms/#overview","title":"Overview","text":"<p>The Search Algorithms project implements classic uninformed and informed search strategies for pathfinding in maze environments. Built using graph search principles, the system explores various frontier management strategies and heuristic functions to navigate Pacman through mazes efficiently while minimizing computational cost.</p> <p>The implementation covers four fundamental search algorithms (DFS, BFS, UCS, A*) and demonstrates their application to complex problems including corner navigation and food collection. The project emphasizes understanding trade-offs between optimality, completeness, and computational efficiency.</p>"},{"location":"search-algorithms/#system-architecture","title":"System Architecture","text":"<p>The search system follows a modular architecture with flexible problem definitions and pluggable algorithms:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      SEARCH ALGORITHMS SYSTEM                                \u2502\n\u2502                        System Workflow Diagram                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502    SEARCH PROBLEM                \u2502\n                    \u2502  - Initial State                 \u2502\n                    \u2502  - Goal Test                     \u2502\n                    \u2502  - Successor Function            \u2502\n                    \u2502  - Cost Function                 \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                          \u2502                            \u2502\n        \u25bc                          \u25bc                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 UNINFORMED     \u2502      \u2502 INFORMED       \u2502        \u2502 HEURISTIC      \u2502\n\u2502 SEARCH         \u2502      \u2502 SEARCH         \u2502        \u2502 FUNCTIONS      \u2502\n\u2502                \u2502      \u2502                \u2502        \u2502                \u2502\n\u2502 - DFS          \u2502      \u2502 - A* Search    \u2502        \u2502 - Manhattan    \u2502\n\u2502 - BFS          \u2502      \u2502                \u2502        \u2502   Distance     \u2502\n\u2502 - UCS          \u2502      \u2502                \u2502        \u2502 - Euclidean    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502 - Custom       \u2502\n         \u2502                       \u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   FRONTIER / FRINGE     \u2502\n                    \u2502  - Stack (DFS)          \u2502\n                    \u2502  - Queue (BFS)          \u2502\n                    \u2502  - Priority Queue (UCS) \u2502\n                    \u2502  - Priority Queue (A*)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   EXPLORED SET          \u2502\n                    \u2502  - Track visited states \u2502\n                    \u2502  - Avoid cycles         \u2502\n                    \u2502  - Graph search         \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   SOLUTION PATH         \u2502\n                    \u2502  - Sequence of actions  \u2502\n                    \u2502  - Path cost            \u2502\n                    \u2502  - Nodes expanded       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nKey Algorithms:\n\u251c\u2500 DFS: Stack-based, explores deep paths first\n\u251c\u2500 BFS: Queue-based, explores level by level\n\u251c\u2500 UCS: Priority queue by path cost\n\u2514\u2500 A*: Priority queue by f(n) = g(n) + h(n)\n</code></pre>"},{"location":"search-algorithms/#features-and-capabilities","title":"Features and Capabilities","text":""},{"location":"search-algorithms/#search-algorithms-implemented","title":"Search Algorithms Implemented","text":""},{"location":"search-algorithms/#1-depth-first-search-dfs","title":"1. Depth-First Search (DFS)","text":"<ul> <li>Strategy: Explore deepest nodes first using Stack (LIFO)</li> <li>Complete: No (can get stuck in infinite branches)</li> <li>Optimal: No (finds any solution, not necessarily shortest)</li> <li>Complexity: Time O(b^m), Space O(bm)</li> <li>Best For: Memory-constrained scenarios, deep solutions</li> </ul>"},{"location":"search-algorithms/#2-breadth-first-search-bfs","title":"2. Breadth-First Search (BFS)","text":"<ul> <li>Strategy: Explore shallowest nodes first using Queue (FIFO)</li> <li>Complete: Yes (finds solution if one exists)</li> <li>Optimal: Yes (if all step costs equal)</li> <li>Complexity: Time O(b^d), Space O(b^d)</li> <li>Best For: Finding shortest paths with uniform costs</li> </ul>"},{"location":"search-algorithms/#3-uniform-cost-search-ucs","title":"3. Uniform Cost Search (UCS)","text":"<ul> <li>Strategy: Expand least-cost node using Priority Queue</li> <li>Complete: Yes (with positive step costs)</li> <li>Optimal: Yes (guaranteed least-cost path)</li> <li>Complexity: Time/Space O(b^(C*/\u03b5))</li> <li>Best For: Variable action costs, optimal solutions required</li> </ul>"},{"location":"search-algorithms/#4-a-search","title":"4. A* Search","text":"<ul> <li>Strategy: Best-first search with f(n) = g(n) + h(n)</li> <li>Complete: Yes (with admissible heuristic)</li> <li>Optimal: Yes (with admissible heuristic)</li> <li>Complexity: Depends on heuristic quality</li> <li>Best For: When good heuristic available, optimal solution needed</li> </ul>"},{"location":"search-algorithms/#problem-types","title":"Problem Types","text":""},{"location":"search-algorithms/#position-search-problem","title":"Position Search Problem","text":"<ul> <li>Goal: Navigate to specific target location</li> <li>State: (x, y) coordinates in maze</li> <li>Actions: North, South, East, West</li> <li>Application: Basic pathfinding</li> </ul>"},{"location":"search-algorithms/#corners-problem","title":"Corners Problem","text":"<ul> <li>Goal: Visit all four maze corners</li> <li>State: (position, visited_corners_set)</li> <li>Actions: North, South, East, West</li> <li>Application: Multiple goal navigation</li> </ul>"},{"location":"search-algorithms/#food-search-problem","title":"Food Search Problem","text":"<ul> <li>Goal: Collect all food pellets efficiently</li> <li>State: (position, remaining_food_grid)</li> <li>Actions: North, South, East, West</li> <li>Application: Resource collection optimization</li> </ul>"},{"location":"search-algorithms/#quick-start","title":"Quick Start","text":""},{"location":"search-algorithms/#installation","title":"Installation","text":"<p>Ensure Python 3.6+ is installed: <pre><code>python --version\n</code></pre></p> <p>All required files are included in the project. No external dependencies needed.</p>"},{"location":"search-algorithms/#running-algorithms","title":"Running Algorithms","text":"<p>Navigate to the project directory: <pre><code>cd 0_search\n</code></pre></p>"},{"location":"search-algorithms/#dfs-on-medium-maze","title":"DFS on Medium Maze","text":"<pre><code>python pacman.py -l mediumMaze -p SearchAgent -a fn=depthFirstSearch\n</code></pre>"},{"location":"search-algorithms/#bfs-on-medium-maze","title":"BFS on Medium Maze","text":"<pre><code>python pacman.py -l mediumMaze -p SearchAgent -a fn=breadthFirstSearch\n</code></pre>"},{"location":"search-algorithms/#ucs-with-variable-costs","title":"UCS with Variable Costs","text":"<pre><code>python pacman.py -l mediumMaze -p SearchAgent -a fn=uniformCostSearch\n</code></pre>"},{"location":"search-algorithms/#a-search-with-manhattan-heuristic","title":"A* Search with Manhattan Heuristic","text":"<pre><code>python pacman.py -l bigMaze -z .5 -p SearchAgent -a fn=astar,heuristic=manhattanHeuristic\n</code></pre>"},{"location":"search-algorithms/#command-line-options","title":"Command Line Options","text":"Option Description Example <code>-l</code> Maze layout <code>tinyMaze</code>, <code>mediumMaze</code>, <code>bigMaze</code> <code>-p</code> Agent type <code>SearchAgent</code> <code>-a</code> Agent arguments <code>fn=astar,heuristic=manhattanHeuristic</code> <code>-z</code> Zoom level <code>0.5</code> (half size), <code>1.0</code> (normal) <code>--frameTime</code> Animation speed <code>0</code> (max speed), <code>0.1</code> (slow) <code>-q</code> Quiet mode No graphics"},{"location":"search-algorithms/#algorithm-details","title":"Algorithm Details","text":""},{"location":"search-algorithms/#depth-first-search-dfs","title":"Depth-First Search (DFS)","text":"<p>Pseudocode: <pre><code>function DFS(problem):\n    frontier = Stack()\n    frontier.push(problem.startState)\n    explored = empty set\n\n    while frontier not empty:\n        state = frontier.pop()\n\n        if problem.isGoal(state):\n            return solution\n\n        if state not in explored:\n            explored.add(state)\n            for successor in problem.getSuccessors(state):\n                frontier.push(successor)\n\n    return failure\n</code></pre></p> <p>Characteristics: - Explores one branch completely before backtracking - Memory efficient (only stores path) - Can find suboptimal solutions - Fast for deep goals</p>"},{"location":"search-algorithms/#breadth-first-search-bfs","title":"Breadth-First Search (BFS)","text":"<p>Pseudocode: <pre><code>function BFS(problem):\n    frontier = Queue()\n    frontier.push(problem.startState)\n    explored = empty set\n\n    while frontier not empty:\n        state = frontier.pop()\n\n        if problem.isGoal(state):\n            return solution\n\n        if state not in explored:\n            explored.add(state)\n            for successor in problem.getSuccessors(state):\n                frontier.push(successor)\n\n    return failure\n</code></pre></p> <p>Characteristics: - Explores all nodes at depth d before depth d+1 - Guarantees shortest path (uniform costs) - Memory intensive (stores all nodes at current level) - Optimal for unit step costs</p>"},{"location":"search-algorithms/#uniform-cost-search-ucs","title":"Uniform Cost Search (UCS)","text":"<p>Pseudocode: <pre><code>function UCS(problem):\n    frontier = PriorityQueue()\n    frontier.push(problem.startState, priority=0)\n    explored = empty set\n\n    while frontier not empty:\n        state = frontier.pop()  # Lowest cost first\n\n        if problem.isGoal(state):\n            return solution\n\n        if state not in explored:\n            explored.add(state)\n            for successor, cost in problem.getSuccessors(state):\n                frontier.push(successor, priority=pathCost + cost)\n\n    return failure\n</code></pre></p> <p>Characteristics: - Always expands least-cost node - Guarantees optimal solution - Handles variable step costs - More nodes expanded than A*</p>"},{"location":"search-algorithms/#a-search","title":"A* Search","text":"<p>Pseudocode: <pre><code>function AStar(problem, heuristic):\n    frontier = PriorityQueue()\n    frontier.push(problem.startState, priority=heuristic(startState))\n    explored = empty set\n\n    while frontier not empty:\n        state = frontier.pop()  # Lowest f(n) first\n\n        if problem.isGoal(state):\n            return solution\n\n        if state not in explored:\n            explored.add(state)\n            for successor, cost in problem.getSuccessors(state):\n                g = pathCost + cost\n                h = heuristic(successor)\n                f = g + h\n                frontier.push(successor, priority=f)\n\n    return failure\n</code></pre></p> <p>Evaluation Function: <pre><code>f(n) = g(n) + h(n)\nwhere:\n  g(n) = actual cost from start to n\n  h(n) = estimated cost from n to goal\n</code></pre></p> <p>Characteristics: - Uses heuristic to guide search - Optimal with admissible heuristic (h \u2264 h*) - Expands fewer nodes than uninformed search - Efficiency depends on heuristic quality</p>"},{"location":"search-algorithms/#heuristic-design","title":"Heuristic Design","text":""},{"location":"search-algorithms/#manhattan-distance-heuristic","title":"Manhattan Distance Heuristic","text":"<p>Formula: <pre><code>h(position, goal) = |x\u2081 - x\u2082| + |y\u2081 - y\u2082|\n</code></pre></p> <p>Properties: - Admissible: Never overestimates (can't move diagonally through walls) - Consistent: h(n) \u2264 c(n,a,n') + h(n') - Efficient: O(1) computation - Use Case: Grid-based pathfinding</p>"},{"location":"search-algorithms/#corners-heuristic","title":"Corners Heuristic","text":"<p>Strategy: Estimate cost to visit all unvisited corners</p> <p>Approach: <pre><code>def cornersHeuristic(state, problem):\n    position, visitedCorners = state\n    unvisited = [c for c in corners if c not in visitedCorners]\n\n    if not unvisited:\n        return 0\n\n    # Distance to farthest unvisited corner\n    distances = [manhattanDistance(position, c) for c in unvisited]\n    return max(distances)\n</code></pre></p> <p>Properties: - Admissible (underestimates true cost) - Guides search toward unvisited corners - Simple but effective</p>"},{"location":"search-algorithms/#food-heuristic","title":"Food Heuristic","text":"<p>Strategy: Estimate cost to collect all remaining food</p> <p>Approach: <pre><code>def foodHeuristic(state, problem):\n    position, foodGrid = state\n    foodList = foodGrid.asList()\n\n    if not foodList:\n        return 0\n\n    # Maximum distance to any food\n    maxDist = max([manhattanDistance(position, food) for food in foodList])\n\n    # Add penalty for food count\n    return maxDist + len(foodList) // 2\n</code></pre></p> <p>Properties: - Balances distance and quantity - Admissible approximation - Scalable to many food pellets</p>"},{"location":"search-algorithms/#performance-analysis","title":"Performance Analysis","text":""},{"location":"search-algorithms/#maze-size-comparison","title":"Maze Size Comparison","text":"<p>Small Maze (tinyMaze):</p> Algorithm Path Cost Nodes Expanded Time DFS 10 15 0.01s BFS 10 15 0.01s UCS 10 15 0.01s A* 10 14 0.01s <p>Medium Maze:</p> Algorithm Path Cost Nodes Expanded Time DFS 130 146 0.02s BFS 68 269 0.03s UCS 68 269 0.03s A* 68 221 0.02s <p>Big Maze:</p> Algorithm Path Cost Nodes Expanded Time DFS 210 390 0.05s BFS 210 620 0.10s UCS 210 620 0.10s A* 210 549 0.08s <p>Key Observations: - DFS: Fast but finds suboptimal paths - BFS/UCS: Optimal but expand many nodes - A*: Best of both worlds - optimal with fewer expansions - Heuristic Impact: A* efficiency directly correlates with heuristic quality</p>"},{"location":"search-algorithms/#usage-examples","title":"Usage Examples","text":""},{"location":"search-algorithms/#basic-pathfinding","title":"Basic Pathfinding","text":"<pre><code># Depth-First Search\npython pacman.py -l mediumMaze -p SearchAgent -a fn=dfs\n\n# Breadth-First Search (optimal for unit costs)\npython pacman.py -l mediumMaze -p SearchAgent -a fn=bfs\n\n# Uniform Cost Search (optimal for variable costs)\npython pacman.py -l mediumDottedMaze -p SearchAgent -a fn=ucs\n\n# A* Search with Manhattan heuristic\npython pacman.py -l bigMaze -z .5 -p SearchAgent -a fn=astar,heuristic=manhattanHeuristic\n</code></pre>"},{"location":"search-algorithms/#complex-problems","title":"Complex Problems","text":"<pre><code># Corners Problem with BFS\npython pacman.py -l mediumCorners -p SearchAgent -a fn=bfs,prob=CornersProblem\n\n# Corners Problem with A* and custom heuristic\npython pacman.py -l mediumCorners -p SearchAgent -a fn=astar,prob=CornersProblem,heuristic=cornersHeuristic\n\n# Food Search Problem\npython pacman.py -l trickySearch -p SearchAgent -a fn=astar,prob=FoodSearchProblem,heuristic=foodHeuristic\n</code></pre>"},{"location":"search-algorithms/#performance-testing","title":"Performance Testing","text":"<pre><code># Maximum speed (no graphics)\npython pacman.py -l bigMaze -p SearchAgent -a fn=astar,heuristic=manhattanHeuristic --frameTime=0 -q\n\n# Slow motion visualization\npython pacman.py -l mediumMaze -p SearchAgent -a fn=bfs --frameTime=0.5\n\n# Multiple runs for statistics\npython pacman.py -l mediumMaze -p SearchAgent -a fn=astar -n 10 -q\n</code></pre>"},{"location":"search-algorithms/#key-algorithms-explained","title":"Key Algorithms Explained","text":""},{"location":"search-algorithms/#graph-search-template","title":"Graph Search Template","text":"<p>All algorithms follow this general structure:</p> <pre><code>def graphSearch(problem, frontierDataStructure):\n    \"\"\"\n    Generic graph search algorithm\n    \"\"\"\n    frontier = frontierDataStructure()\n    frontier.push((problem.getStartState(), [], 0))  # (state, actions, cost)\n    explored = set()\n\n    while not frontier.isEmpty():\n        state, actions, cost = frontier.pop()\n\n        # Goal test\n        if problem.isGoalState(state):\n            return actions\n\n        # Avoid revisiting\n        if state not in explored:\n            explored.add(state)\n\n            # Expand successors\n            for successor, action, stepCost in problem.getSuccessors(state):\n                if successor not in explored:\n                    newActions = actions + [action]\n                    newCost = cost + stepCost\n                    frontier.push((successor, newActions, newCost))\n\n    return []  # No solution found\n</code></pre>"},{"location":"search-algorithms/#a-search-with-heuristics","title":"A* Search with Heuristics","text":"<p>Core Equation: <pre><code>f(n) = g(n) + h(n)\n\nwhere:\n  f(n) = estimated total cost of path through n\n  g(n) = actual cost from start to n\n  h(n) = estimated cost from n to goal (heuristic)\n</code></pre></p> <p>Admissibility Requirement: <pre><code>h(n) \u2264 h*(n)  (never overestimate)\n</code></pre></p> <p>Consistency Requirement: <pre><code>h(n) \u2264 c(n, a, n') + h(n')  (triangle inequality)\n</code></pre></p> <p>Implementation: <pre><code>def aStarSearch(problem, heuristic):\n    frontier = PriorityQueue()\n    explored = set()\n\n    startState = problem.getStartState()\n    startPriority = 0 + heuristic(startState, problem)\n    frontier.push((startState, [], 0), startPriority)\n\n    while not frontier.isEmpty():\n        state, actions, cost = frontier.pop()\n\n        if problem.isGoalState(state):\n            return actions\n\n        if state not in explored:\n            explored.add(state)\n\n            for successor, action, stepCost in problem.getSuccessors(state):\n                if successor not in explored:\n                    newActions = actions + [action]\n                    g = cost + stepCost\n                    h = heuristic(successor, problem)\n                    f = g + h\n                    frontier.push((successor, newActions, g), f)\n\n    return []\n</code></pre></p>"},{"location":"search-algorithms/#heuristic-examples","title":"Heuristic Examples","text":""},{"location":"search-algorithms/#manhattan-distance-position-search","title":"Manhattan Distance (Position Search)","text":"<pre><code>def manhattanHeuristic(position, problem):\n    \"\"\"\n    Manhattan distance to goal\n    Admissible for grid navigation without diagonal moves\n    \"\"\"\n    xy1 = position\n    xy2 = problem.goal\n    return abs(xy1[0] - xy2[0]) + abs(xy1[1] - xy2[1])\n</code></pre>"},{"location":"search-algorithms/#euclidean-distance","title":"Euclidean Distance","text":"<pre><code>def euclideanHeuristic(position, problem):\n    \"\"\"\n    Straight-line distance to goal\n    Admissible but less informative than Manhattan for grids\n    \"\"\"\n    xy1 = position\n    xy2 = problem.goal\n    return ((xy1[0] - xy2[0]) ** 2 + (xy1[1] - xy2[1]) ** 2) ** 0.5\n</code></pre>"},{"location":"search-algorithms/#corners-heuristic_1","title":"Corners Heuristic","text":"<pre><code>def cornersHeuristic(state, problem):\n    \"\"\"\n    Heuristic for visiting all corners\n    Uses maximum distance to any unvisited corner\n    \"\"\"\n    position, visitedCorners = state\n    corners = problem.corners\n\n    unvisited = [c for c in corners if c not in visitedCorners]\n\n    if not unvisited:\n        return 0\n\n    # Max distance provides admissible lower bound\n    distances = [manhattanDistance(position, corner) for corner in unvisited]\n    return max(distances)\n</code></pre>"},{"location":"search-algorithms/#food-heuristic_1","title":"Food Heuristic","text":"<pre><code>def foodHeuristic(state, problem):\n    \"\"\"\n    Heuristic for collecting all food\n    Combines distance and quantity factors\n    \"\"\"\n    position, foodGrid = state\n    foodList = foodGrid.asList()\n\n    if not foodList:\n        return 0\n\n    # Distance to farthest food\n    maxDist = max([manhattanDistance(position, food) for food in foodList])\n\n    # Penalty for remaining food count\n    foodPenalty = len(foodList) // 2\n\n    return maxDist + foodPenalty\n</code></pre>"},{"location":"search-algorithms/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"search-algorithms/#automated-testing","title":"Automated Testing","text":"<pre><code># Test all questions\npython autograder.py\n\n# Test specific questions\npython autograder.py -q q1  # DFS\npython autograder.py -q q2  # BFS\npython autograder.py -q q3  # UCS\npython autograder.py -q q4  # A*\npython autograder.py -q q5  # Corners Problem\npython autograder.py -q q6  # Corners Heuristic\npython autograder.py -q q7  # Food Heuristic\npython autograder.py -q q8  # Suboptimal Search\n</code></pre>"},{"location":"search-algorithms/#manual-testing","title":"Manual Testing","text":"<pre><code># Visual testing on different maze sizes\npython pacman.py -l tinyMaze -p SearchAgent -a fn=bfs\npython pacman.py -l mediumMaze -p SearchAgent -a fn=astar\npython pacman.py -l bigMaze -z .5 -p SearchAgent -a fn=astar\n\n# Performance comparison\nfor algo in dfs bfs ucs astar; do\n    echo \"Testing $algo\"\n    python pacman.py -l mediumMaze -p SearchAgent -a fn=$algo -q\ndone\n</code></pre>"},{"location":"search-algorithms/#troubleshooting","title":"Troubleshooting","text":""},{"location":"search-algorithms/#common-issues","title":"Common Issues","text":""},{"location":"search-algorithms/#dfs-goes-in-circles","title":"DFS Goes in Circles","text":"<p>Problem: Infinite loop or very long paths</p> <p>Solution: Ensure explored set prevents revisiting: <pre><code>if state not in explored:\n    explored.add(state)\n    # expand successors\n</code></pre></p>"},{"location":"search-algorithms/#bfs-runs-out-of-memory","title":"BFS Runs Out of Memory","text":"<p>Problem: Too many nodes stored</p> <p>Solutions: 1. Verify explored set works correctly 2. Try smaller maze first 3. Check state representation is hashable</p>"},{"location":"search-algorithms/#a-not-finding-optimal-path","title":"A* Not Finding Optimal Path","text":"<p>Problem: Returns suboptimal solution</p> <p>Solutions: 1. Verify heuristic is admissible (never overestimates) 2. Check f(n) = g(n) + h(n) calculation 3. Ensure priority queue uses f(n) as priority</p>"},{"location":"search-algorithms/#heuristic-returns-incorrect-values","title":"Heuristic Returns Incorrect Values","text":"<p>Problem: Crashes or poor performance</p> <p>Solutions: 1. Never return infinity or negative values 2. Verify admissibility (h \u2264 true cost) 3. Test heuristic in isolation before integration</p>"},{"location":"search-algorithms/#performance-optimization-tips","title":"Performance Optimization Tips","text":"<ol> <li>Choose Right Algorithm:</li> <li>Need optimal? Use A* or UCS</li> <li>Limited memory? Use DFS</li> <li> <p>Uniform costs? Use BFS</p> </li> <li> <p>Design Better Heuristics:</p> </li> <li>More informed = fewer expansions</li> <li>Must remain admissible for optimality</li> <li> <p>Precompute when possible</p> </li> <li> <p>State Representation:</p> </li> <li>Make states hashable for explored set</li> <li>Minimize state size for memory efficiency</li> <li> <p>Include only relevant information</p> </li> <li> <p>Implementation:</p> </li> <li>Check explored before expanding</li> <li>Store path incrementally, not by copying</li> <li>Use appropriate data structures (Stack/Queue/PriorityQueue)</li> </ol>"},{"location":"search-algorithms/#learning-objectives","title":"Learning Objectives","text":"<p>\u2705 Algorithm Implementation - Implement DFS, BFS, UCS, and A* from scratch - Understand frontier and explored set management - Handle state representation and goal testing</p> <p>\u2705 Heuristic Design - Design admissible heuristics for various problems - Balance informativeness with computational cost - Verify consistency and admissibility properties</p> <p>\u2705 Complexity Analysis - Analyze time and space complexity - Compare algorithm performance empirically - Understand optimality and completeness guarantees</p> <p>\u2705 Problem Formulation - Define state spaces for search problems - Design successor functions and cost models - Formulate complex multi-goal problems</p>"},{"location":"search-algorithms/#additional-resources","title":"Additional Resources","text":""},{"location":"search-algorithms/#searchproblem-interface","title":"SearchProblem Interface","text":"<pre><code>problem.getStartState()           # Returns initial state\nproblem.isGoalState(state)        # Returns True if state is goal\nproblem.getSuccessors(state)      # Returns [(successor, action, cost), ...]\n</code></pre>"},{"location":"search-algorithms/#utility-functions","title":"Utility Functions","text":"<pre><code>util.manhattanDistance(xy1, xy2)  # L1 distance\nutil.Stack()                      # LIFO data structure\nutil.Queue()                      # FIFO data structure\nutil.PriorityQueue()              # Ordered by priority\n</code></pre>"},{"location":"search-algorithms/#references","title":"References","text":"<ul> <li>UC Berkeley CS188 - Original project framework</li> <li>Russell &amp; Norvig - \"Artificial Intelligence: A Modern Approach\"</li> <li>Course Materials - CS-5100 lecture slides and readings</li> </ul>"}]}